{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import tiktoken\n",
        "import google.generativeai as genai\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from openai import OpenAI\n",
        "import math\n",
        "\n",
        "\n",
        "BASE_FEATURE_DIM = 384  # Dimension for question embeddings\n",
        "ANSWER_EMBED_DIM = 384  # Dimension for answer embeddings\n",
        "CASCADE_LENGTH = 4      # Number of attempts in the cascade (K)\n",
        "UPDATE_FREQUENCY = 1    # Update JSON records after every question\n",
        "USE_EMBEDDINGS = True   # Use embeddings instead of TF-IDF\n",
        "\n",
        "# LinUCB parameters\n",
        "ALPHA = 0.675           # Exploration-exploitation trade-off parameter\n",
        "LAMBDA_REG = 0.45       # Regularization parameter for LinUCB matrix initialization\n",
        "\n",
        "# Value Density & Cost Estimation parameters\n",
        "BETA_COST = 1.0         # Controls width of LCB interval for LLM output cost estimation\n",
        "EPSILON_VD = 1e-10      # Small positive value for value density calculation\n",
        "DEFAULT_FALLBACK_COST = 0.0001  # Default cost when insufficient data"
      ],
      "metadata": {
        "id": "U9wlFlmndmcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENROUTER_API_KEY = # Input Openrouter API key hr\n",
        "OPENROUTER_BASE_URL = # Input Openrouter URL\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "   \"mistralai/mistral-small-3.1-24b-instruct\": {\"input_cost\": 0.05 / 1e6, \"output_cost\": 0.15 / 1e6},\n",
        "    \"microsoft/phi-4\": {\"input_cost\": 0.07 / 1e6, \"output_cost\": 0.14 / 1e6},\n",
        "    \"meta-llama/llama-4-maverick\": {\"input_cost\": 0.17 / 1e6, \"output_cost\": 0.16 / 1e6},\n",
        "    \"google/gemini-2.0-flash-001\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"openai/gpt-4.1-nano\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"deepseek/deepseek-chat\": {\"input_cost\": 0.38 / 1e6, \"output_cost\": 0.89 / 1e6},\n",
        "}"
      ],
      "metadata": {
        "id": "NK2dCs_Rd9t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtsTphYkdJ1H"
      },
      "outputs": [],
      "source": [
        "GRADER_MODEL_NAME = \"google/gemini-2.0-flash-lite-001\"\n",
        "\n",
        "AVAILABLE_LLMS = list(MODELS_CONFIG.keys())\n",
        "LLM_ID_DIM = len(AVAILABLE_LLMS)\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=OPENROUTER_BASE_URL,\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        "    # You might need to add default headers if required by your OpenRouter setup/account\n",
        "    # default_headers={\"HTTP-Referer\": \"YOUR_SITE_URL\", \"X-Title\": \"YOUR_APP_NAME\"}\n",
        ")\n",
        "INPUT_JSON = \"Math500.json\"\n",
        "RECORDS_PATH = \"M92V2.json\"  # The name of output file is called M92\n",
        "LINUCB_MODEL_PATH = \"M92V2.npz\"\n",
        "SUMMARY_STATS_PATH = \"M92V2.txt\"\n",
        "BACKUP_SUFFIX = \".bak\"  # For backing up existing files\n",
        "\n",
        "CONTEXT_FEATURE_DIM = ANSWER_EMBED_DIM + LLM_ID_DIM + ANSWER_EMBED_DIM\n",
        "TOTAL_FEATURE_DIM = BASE_FEATURE_DIM + 1 + CONTEXT_FEATURE_DIM\n",
        "\n",
        "class ModelCostEstimator:\n",
        "    def __init__(self, model_names):\n",
        "        self.stats = {}\n",
        "        for model_name in model_names:\n",
        "            self.stats[model_name] = {\n",
        "                \"overall\": {\"count\": 0, \"total_output_cost\": 0.0, \"sum_sq_output_cost\": 0.0},\n",
        "                \"by_position\": {\n",
        "                    str(i): {\"count\": 0, \"total_output_cost\": 0.0, \"sum_sq_output_cost\": 0.0}\n",
        "                    for i in range(1, CASCADE_LENGTH + 1)\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def update_stats(self, model_name, step, actual_output_cost):\n",
        "        step_str = str(step)\n",
        "        if model_name in self.stats and step_str in self.stats[model_name][\"by_position\"]:\n",
        "            pos_stats = self.stats[model_name][\"by_position\"][step_str]\n",
        "            pos_stats[\"count\"] += 1\n",
        "            pos_stats[\"total_output_cost\"] += actual_output_cost\n",
        "            pos_stats[\"sum_sq_output_cost\"] += actual_output_cost ** 2\n",
        "\n",
        "        if model_name in self.stats:\n",
        "            overall_stats = self.stats[model_name][\"overall\"]\n",
        "            overall_stats[\"count\"] += 1\n",
        "            overall_stats[\"total_output_cost\"] += actual_output_cost\n",
        "            overall_stats[\"sum_sq_output_cost\"] += actual_output_cost ** 2\n",
        "\n",
        "    def estimate_lcb_output_cost(self, model_name, step, beta=BETA_COST, default_cost=DEFAULT_FALLBACK_COST):\n",
        "        \"\"\"Estimate the Lower Confidence Bound (LCB) of output cost for a model at a specific step.\"\"\"\n",
        "        step_str = str(step)\n",
        "\n",
        "        # Try position-specific stats first\n",
        "        if model_name in self.stats and step_str in self.stats[model_name][\"by_position\"]:\n",
        "            pos_stats = self.stats[model_name][\"by_position\"][step_str]\n",
        "            if pos_stats[\"count\"] >= 2:\n",
        "                mean_cost = pos_stats[\"total_output_cost\"] / pos_stats[\"count\"]\n",
        "                variance = (pos_stats[\"sum_sq_output_cost\"] / pos_stats[\"count\"]) - (mean_cost ** 2)\n",
        "                variance = max(0, variance)  # Ensure non-negative variance\n",
        "                std_dev = math.sqrt(variance)\n",
        "                std_error = std_dev / math.sqrt(pos_stats[\"count\"])\n",
        "                lcb_cost = mean_cost - beta * std_error\n",
        "                return max(EPSILON_VD / 10, lcb_cost)  # Ensure positive cost for value density\n",
        "\n",
        "        # Fallback to overall stats if position-specific stats are insufficient\n",
        "        if model_name in self.stats:\n",
        "            overall_stats = self.stats[model_name][\"overall\"]\n",
        "            if overall_stats[\"count\"] >= 2:\n",
        "                mean_cost = overall_stats[\"total_output_cost\"] / overall_stats[\"count\"]\n",
        "                variance = (overall_stats[\"sum_sq_output_cost\"] / overall_stats[\"count\"]) - (mean_cost ** 2)\n",
        "                variance = max(0, variance)\n",
        "                std_dev = math.sqrt(variance)\n",
        "                std_error = std_dev / math.sqrt(overall_stats[\"count\"])\n",
        "                lcb_cost = mean_cost - beta * std_error\n",
        "                return max(EPSILON_VD / 10, lcb_cost)\n",
        "\n",
        "        # Use default cost if there is insufficient data\n",
        "        return default_cost\n",
        "\n",
        "    def save_state(self, file_path):\n",
        "        try:\n",
        "            save_dict = {}\n",
        "            for model_name, model_stats in self.stats.items():\n",
        "                overall = model_stats[\"overall\"]\n",
        "                save_dict[f\"overall_count_{model_name}\"] = overall[\"count\"]\n",
        "                save_dict[f\"overall_total_output_cost_{model_name}\"] = overall[\"total_output_cost\"]\n",
        "                save_dict[f\"overall_sum_sq_output_cost_{model_name}\"] = overall[\"sum_sq_output_cost\"]\n",
        "                for pos, pos_stats in model_stats[\"by_position\"].items():\n",
        "                    save_dict[f\"pos{pos}_count_{model_name}\"] = pos_stats[\"count\"]\n",
        "                    save_dict[f\"pos{pos}_total_output_cost_{model_name}\"] = pos_stats[\"total_output_cost\"]\n",
        "                    save_dict[f\"pos{pos}_sum_sq_output_cost_{model_name}\"] = pos_stats[\"sum_sq_output_cost\"]\n",
        "            np.savez_compressed(file_path, **save_dict)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_state(self, file_path):\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            for model_name in self.stats:\n",
        "                if f\"overall_count_{model_name}\" in loaded:\n",
        "                    overall = self.stats[model_name][\"overall\"]\n",
        "                    overall[\"count\"] = int(loaded[f\"overall_count_{model_name}\"])\n",
        "                    overall[\"total_output_cost\"] = float(loaded[f\"overall_total_output_cost_{model_name}\"])\n",
        "                    overall[\"sum_sq_output_cost\"] = float(loaded[f\"overall_sum_sq_output_cost_{model_name}\"])\n",
        "                for pos in self.stats[model_name][\"by_position\"]:\n",
        "                    if f\"pos{pos}_count_{model_name}\" in loaded:\n",
        "                        pos_stats = self.stats[model_name][\"by_position\"][pos]\n",
        "                        pos_stats[\"count\"] = int(loaded[f\"pos{pos}_count_{model_name}\"])\n",
        "                        pos_stats[\"total_output_cost\"] = float(loaded[f\"pos{pos}_total_output_cost_{model_name}\"])\n",
        "                        pos_stats[\"sum_sq_output_cost\"] = float(loaded[f\"pos{pos}_sum_sq_output_cost_{model_name}\"])\n",
        "            print(f\"Loaded model cost estimator state from {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, feature_dim=BASE_FEATURE_DIM, use_embeddings=USE_EMBEDDINGS):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.use_embeddings = use_embeddings\n",
        "        if use_embeddings:\n",
        "            try:\n",
        "                self.embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
        "                print(\"Initialized sentence transformer embedding model\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error initializing sentence transformer: {e}\\nFalling back to TF-IDF.\")\n",
        "                self.use_embeddings = False\n",
        "        if not self.use_embeddings:\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.svd = None\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, questions):\n",
        "        if not self.use_embeddings:\n",
        "            from sklearn.decomposition import TruncatedSVD\n",
        "            all_text = [q[\"question\"] for q in questions]\n",
        "            self.vectorizer.fit(all_text)\n",
        "            dtm = self.vectorizer.transform(all_text)\n",
        "            n_components = min(self.feature_dim, dtm.shape[1])\n",
        "            self.svd = TruncatedSVD(n_components=n_components)\n",
        "            self.svd.fit(dtm)\n",
        "            print(f\"Using TF-IDF with SVD dimensionality reduction to {n_components} features\")\n",
        "            print(f\"Explained variance ratio: {sum(self.svd.explained_variance_ratio_):.4f}\")\n",
        "        self.initialized = True\n",
        "\n",
        "    def extract_features(self, question):\n",
        "        \"\"\"Extract features from a question (no options for Math500).\"\"\"\n",
        "        if not self.initialized:\n",
        "            raise ValueError(\"Feature extractor not initialized. Call initialize() first.\")\n",
        "        text = question[\"question\"]\n",
        "        if self.use_embeddings:\n",
        "            features = self.embedding_model.encode([text])[0]\n",
        "        else:\n",
        "            tfidf_vector = self.vectorizer.transform([text])\n",
        "            features = self.svd.transform(tfidf_vector)[0]\n",
        "            # Ensure we have exactly feature_dim dimensions\n",
        "            if len(features) < self.feature_dim:\n",
        "                padding = np.zeros(self.feature_dim - len(features))\n",
        "                features = np.concatenate([features, padding])\n",
        "        return features\n",
        "\n",
        "    def extract_answer_features(self, answer_text):\n",
        "        if not answer_text:\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "        if self.use_embeddings:\n",
        "            try:\n",
        "                features = self.embedding_model.encode([answer_text])[0]\n",
        "                if features is None:\n",
        "                    print(f\"Warning: Embedding for answer text resulted in None. Returning zero vector.\")\n",
        "                    return np.zeros(ANSWER_EMBED_DIM)\n",
        "                # Handle if the embedding doesn't match the expected dimension\n",
        "                if len(features) != ANSWER_EMBED_DIM:\n",
        "                    features = features[:ANSWER_EMBED_DIM] if len(features) > ANSWER_EMBED_DIM else np.concatenate([features, np.zeros(ANSWER_EMBED_DIM - len(features))])\n",
        "                return features\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding answer: {e}\")\n",
        "                return np.zeros(ANSWER_EMBED_DIM)\n",
        "        else:\n",
        "            # For simplicity, use zero vector if not using embeddings\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "    def construct_feature_vector(self, base_features, step_i, failed_answers, failed_llm_ids, model_name_to_index):\n",
        "        \"\"\"\n",
        "        Construct the augmented feature vector for LinUCB.\n",
        "        - base_features: Features extracted from the question\n",
        "        - step_i: Current step in the cascade (1-indexed)\n",
        "        - failed_answers: List of previous failed answer strings\n",
        "        - failed_llm_ids: List of previous failed LLM IDs\n",
        "        - model_name_to_index: Mapping from model names to indices\n",
        "        \"\"\"\n",
        "        normalized_step = np.array([step_i / CASCADE_LENGTH])\n",
        "\n",
        "        # Initialize context features - last answer embedding\n",
        "        last_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        if step_i > 1 and failed_answers:\n",
        "            last_answer_features = self.extract_answer_features(failed_answers[-1])\n",
        "\n",
        "        # Calculate Last Failed LLM ID One-Hot encoding\n",
        "        last_llm_onehot = np.zeros(LLM_ID_DIM)\n",
        "        if step_i > 1 and failed_llm_ids:\n",
        "            last_llm_name = failed_llm_ids[-1]\n",
        "            if last_llm_name in model_name_to_index:\n",
        "                last_llm_onehot[model_name_to_index[last_llm_name]] = 1.0\n",
        "\n",
        "        # Calculate Average Failed Answer Embedding\n",
        "        avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        if step_i > 1 and failed_answers:\n",
        "            all_answer_features = [self.extract_answer_features(ans) for ans in failed_answers]\n",
        "            if all_answer_features:\n",
        "                avg_answer_features = np.mean(all_answer_features, axis=0)\n",
        "        if avg_answer_features.shape == (): # Handle case where avg_answer_features might be a scalar\n",
        "            avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "        context_features = np.concatenate([last_answer_features, last_llm_onehot, avg_answer_features])\n",
        "        augmented_features = np.concatenate([base_features, normalized_step, context_features])\n",
        "\n",
        "        # Ensure the final dimension matches expectations\n",
        "        if augmented_features.shape[0] != TOTAL_FEATURE_DIM:\n",
        "            raise ValueError(f\"Constructed feature vector dimension {augmented_features.shape[0]} != expected {TOTAL_FEATURE_DIM}\")\n",
        "        return augmented_features\n",
        "\n",
        "\n",
        "class LinUCBModel:\n",
        "    def __init__(self, model_names, feature_dim=TOTAL_FEATURE_DIM, alpha=ALPHA, lambda_reg=LAMBDA_REG):\n",
        "        self.model_names = model_names\n",
        "        self.feature_dim = feature_dim\n",
        "        self.alpha = alpha\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.model_name_to_index = {name: i for i, name in enumerate(model_names)}\n",
        "        self.cost_estimator = ModelCostEstimator(model_names)\n",
        "        self.models = {\n",
        "            model_name: {\n",
        "                'A': np.identity(feature_dim) * lambda_reg,\n",
        "                'b': np.zeros(feature_dim),\n",
        "                'last_call_time': 0\n",
        "            } for model_name in model_names\n",
        "        }\n",
        "\n",
        "    def update_reward_only(self, model_name, feature_vector, reward):\n",
        "        \"\"\"Update the LinUCB model parameters based on observed reward.\"\"\"\n",
        "        model = self.models[model_name]\n",
        "        model['A'] += np.outer(feature_vector, feature_vector)\n",
        "        model['b'] += feature_vector * reward\n",
        "\n",
        "    def calculate_ucb_scores(self, feature_vector):\n",
        "        \"\"\"Calculate UCB scores for model selection.\"\"\"\n",
        "        scores = {}\n",
        "        for model_name in self.model_names:\n",
        "            model = self.models[model_name]\n",
        "            try:\n",
        "                A_inv = np.linalg.inv(model['A'])\n",
        "                theta = A_inv.dot(model['b'])\n",
        "                ucb_term = self.alpha * np.sqrt(feature_vector.dot(A_inv).dot(feature_vector))\n",
        "                expected_reward = feature_vector.dot(theta)\n",
        "                scores[model_name] = {\n",
        "                    \"p_ia\": float(expected_reward),\n",
        "                    \"e_ia\": float(ucb_term),\n",
        "                    \"ucb_score\": float(expected_reward + ucb_term)\n",
        "                }\n",
        "            except np.linalg.LinAlgError:\n",
        "                scores[model_name] = {\"p_ia\": 0.0, \"e_ia\": 0.0, \"ucb_score\": 0.0}\n",
        "        return scores\n",
        "\n",
        "    def select_model_value_density(self, feature_vector, step, prompt, remaining_budget, is_test_phase, reward_importance, cost_importance, used_llms_in_current_cascade=None):\n",
        "        \"\"\"Select a model based on strategy: pure UCB in train, weighted score in test.\"\"\"\n",
        "        scores = self.calculate_ucb_scores(feature_vector)\n",
        "        if not scores:\n",
        "            return None, {}\n",
        "        if used_llms_in_current_cascade is None:\n",
        "            used_llms_in_current_cascade = []\n",
        "\n",
        "        best_model = None\n",
        "        max_selection_score = -float('inf')\n",
        "\n",
        "        for model_name, score_info in scores.items():\n",
        "            # This except block is expected for OpenRouter models listed in MODELS_CONFIG\n",
        "            try:\n",
        "                deterministic_input_cost = len(prompt) // 4 * MODELS_CONFIG[model_name][\"input_cost\"]\n",
        "            except Exception:\n",
        "                print(f\"Warning: Input cost configuration missing for {model_name}. Using default.\")\n",
        "                deterministic_input_cost = len(prompt) // 4 * DEFAULT_FALLBACK_COST\n",
        "\n",
        "            lcb_output_cost_estimate = self.cost_estimator.estimate_lcb_output_cost(model_name, step)\n",
        "            score_info['total_estimated_cost'] = float(deterministic_input_cost + lcb_output_cost_estimate)\n",
        "            # 'remaining_budget' and 'budget_sufficient' will be updated per model inside phase logic\n",
        "\n",
        "        # --- Phase-specific selection logic ---\n",
        "        if not is_test_phase:\n",
        "            # TRAIN PHASE: Pure UCB selection\n",
        "            for model_name, score_info in scores.items():\n",
        "                score_info['budget_sufficient'] = True # Budget is effectively infinite for selection\n",
        "                current_model_selection_score = -float('inf')\n",
        "                if model_name in used_llms_in_current_cascade:\n",
        "                    score_info['selection_strategy'] = 'Prohibited (Train - Re-selection)'\n",
        "                else:\n",
        "                    current_model_selection_score = score_info[\"ucb_score\"]\n",
        "                    score_info['selection_strategy'] = 'Pure UCB (Train)'\n",
        "                score_info['selection_score'] = float(current_model_selection_score)\n",
        "                if current_model_selection_score > max_selection_score:\n",
        "                    max_selection_score = current_model_selection_score\n",
        "                    best_model = model_name\n",
        "        else:\n",
        "            # TEST PHASE: Budget-aware, value-density selection\n",
        "            # Stage A: Identify budget-eligible models\n",
        "            eligible_models_data_list = []\n",
        "            for model_name, score_info in scores.items():\n",
        "                score_info['remaining_budget'] = float(remaining_budget)\n",
        "                if model_name in used_llms_in_current_cascade:\n",
        "                    score_info['selection_strategy'] = 'Prohibited (Test - Re-selection)'\n",
        "                    score_info['selection_score'] = -float('inf')\n",
        "                    continue\n",
        "                if score_info['total_estimated_cost'] <= remaining_budget:\n",
        "                    score_info['budget_sufficient'] = True\n",
        "                    eligible_models_data_list.append((model_name, score_info['total_estimated_cost'], score_info))\n",
        "                else:\n",
        "                    score_info['budget_sufficient'] = False\n",
        "                    score_info['selection_strategy'] = 'Budget Pruned (Test)'\n",
        "                    score_info['selection_score'] = -float('inf')\n",
        "\n",
        "            if not eligible_models_data_list:\n",
        "                return None, scores # All models were pruned by budget or re-selection\n",
        "\n",
        "            # Stage B: Find max_estimated_cost among ELIGIBLE models for normalization\n",
        "            all_eligible_costs = [cost for _, cost, _ in eligible_models_data_list]\n",
        "            max_eligible_cost = max(all_eligible_costs) if all_eligible_costs else EPSILON_VD\n",
        "            max_eligible_cost = max(max_eligible_cost, EPSILON_VD) # Safeguard against zero cost\n",
        "\n",
        "            # Stage C: Calculate final selection scores for ELIGIBLE models\n",
        "            for model_name, model_cost, score_info_ref in eligible_models_data_list:\n",
        "                ucb_score = score_info_ref[\"ucb_score\"]\n",
        "                cost_norm_factor = 1 + ((model_cost + EPSILON_VD) / (max_eligible_cost + EPSILON_VD))\n",
        "                stable_denominator = 1.0 + max(cost_norm_factor, EPSILON_VD)\n",
        "                current_model_selection_score = ucb_score / stable_denominator\n",
        "\n",
        "                score_info_ref['selection_strategy'] = 'Double/Norm (Test)'\n",
        "                score_info_ref['selection_score_terms'] = {\n",
        "                    'ucb_score_numerator': float(ucb_score),\n",
        "                    'stable_denominator_used': float(stable_denominator),\n",
        "                }\n",
        "                score_info_ref['selection_score'] = float(current_model_selection_score)\n",
        "\n",
        "                if current_model_selection_score > max_selection_score:\n",
        "                    max_selection_score = current_model_selection_score\n",
        "                    best_model = model_name\n",
        "\n",
        "        return best_model, scores\n",
        "\n",
        "    def register_model_call(self, model_name):\n",
        "        self.models[model_name]['last_call_time'] = time.time()\n",
        "\n",
        "    def respect_rate_limit(self, model_name):\n",
        "        \"\"\"Wait if necessary to respect the model's rate limit.\"\"\"\n",
        "        # For the specified OpenRouter models, 'rpm' is not defined, so this block will be skipped.\n",
        "        model_cfg = MODELS_CONFIG.get(model_name)\n",
        "        if model_cfg and \"rpm\" in model_cfg:\n",
        "            model_state = self.models[model_name]\n",
        "            min_seconds_between_calls = 60.0 / model_cfg[\"rpm\"]\n",
        "            time_since_last_call = time.time() - model_state['last_call_time']\n",
        "            if time_since_last_call < min_seconds_between_calls:\n",
        "                time.sleep(min_seconds_between_calls - time_since_last_call)\n",
        "\n",
        "    def save_model_state(self, file_path):\n",
        "        \"\"\"Save the model state to a file using numpy's compressed format.\"\"\"\n",
        "        save_dict = {f'A_{model_name}': model['A'] for model_name, model in self.models.items()}\n",
        "        for model_name, model in self.models.items():\n",
        "            save_dict[f'b_{model_name}'] = model['b']\n",
        "        np.savez_compressed(file_path, **save_dict)\n",
        "        # Save cost estimator state\n",
        "        self.cost_estimator.save_state(file_path.replace('.npz', '_cost_estimator.npz'))\n",
        "\n",
        "    def load_model_state(self, file_path):\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            for model_name in self.models.keys():\n",
        "                if f'A_{model_name}' in loaded and f'b_{model_name}' in loaded:\n",
        "                    self.models[model_name]['A'] = loaded[f'A_{model_name}']\n",
        "                    self.models[model_name]['b'] = loaded[f'b_{model_name}']\n",
        "            print(f\"Loaded LinUCB model state from {file_path}\")\n",
        "            # Load cost estimator state\n",
        "            cost_estimator_path = file_path.replace('.npz', '_cost_estimator.npz')\n",
        "            if os.path.exists(cost_estimator_path):\n",
        "                self.cost_estimator.load_state(cost_estimator_path)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class BatchBudgetCascade:\n",
        "    def __init__(self, feature_extractor, linucb_model, cascade_length=CASCADE_LENGTH):\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.linucb_model = linucb_model\n",
        "        self.cascade_length = cascade_length\n",
        "\n",
        "    def format_prompt(self, question, failed_answers=None, failed_llm_ids=None):\n",
        "        prompt = f\"Solve the following math problem: {question['question']}\\n\\n\"\n",
        "        prompt += \"Also provide an explnation in one/serveral very short yet consice complete sentence within 75 words in total.\\n\"\n",
        "        prompt += \"At the end, clearly state your final answer in LaTeX format, enclosed within \\\\boxed{}.\\n\"\n",
        "        prompt += \"For example: 'The final answer is \\\\boxed{x=5}'.\"\n",
        "        if failed_answers and failed_llm_ids:\n",
        "            prompt += \"\\n\\nNote: The following previous attempts were incorrect. Please provide a different solution:\\n\"\n",
        "            for i, answer_info in enumerate(failed_answers):\n",
        "                prompt += f\"- Attempt {i+1} (by {failed_llm_ids[i]}) led to: {answer_info}\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def parse_llm_answer(self, answer_text):\n",
        "        \"\"\"\n",
        "        Modified: Returns the stripped raw text as the answer for grading,\n",
        "        and the full raw text as the explanation (matching Code 1's behavior).\n",
        "        \"\"\"\n",
        "        if not answer_text:\n",
        "            return \"\", \"\"\n",
        "        return answer_text.strip(), answer_text\n",
        "\n",
        "    def grade_with_gemma12b(self, llm_answer_latex, ground_truth_latex):\n",
        "        \"\"\"Grade an answer against the ground truth using the grader model.\"\"\"\n",
        "        if llm_answer_latex is None or llm_answer_latex == ground_truth_latex:\n",
        "            return llm_answer_latex is not None\n",
        "\n",
        "        prompt = f\"Expression 1: {llm_answer_latex}\\nExpression 2: {ground_truth_latex}\\n\\n\"\n",
        "        prompt += \"Expression 2 is the answer and expression is attempt by student,  look at their final answer only which might be boxed, does student get the final expected answer？ Respond with only the word 'True' or 'False'.\"\n",
        "        try:\n",
        "            time.sleep(0.5)  # Simple rate limiting for the grader\n",
        "            api_response = openrouter_client.chat.completions.create(\n",
        "                model=GRADER_MODEL_NAME,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0, # Keep it deterministic for grading\n",
        "                max_tokens=10\n",
        "            )\n",
        "            grader_response_text = api_response.choices[0].message.content.strip().lower()\n",
        "            if \"true\" in grader_response_text:\n",
        "                return True\n",
        "            elif \"false\" in grader_response_text:\n",
        "                return False\n",
        "            else:\n",
        "                print(f\"Warning: Grader returned ambiguous response: {grader_response_text}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling grader model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def calculate_token_cost(self, model_name, prompt, response_text, usage_info=None):\n",
        "        \"\"\"Calculate the actual cost. For OpenRouter, relies on usage_info from API response.\"\"\"\n",
        "        input_tokens, output_tokens, total_cost = 0, 0, 0.0\n",
        "        error_message = None\n",
        "        if model_name in MODELS_CONFIG:\n",
        "            model_cfg = MODELS_CONFIG[model_name]\n",
        "            if usage_info:\n",
        "                input_tokens = usage_info.get(\"prompt_tokens\", 0)\n",
        "                output_tokens = usage_info.get(\"completion_tokens\", 0)\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "            else:\n",
        "                error_message = f\"Usage info not available for {model_name}. Cost is a rough estimate.\"\n",
        "                input_tokens = len(prompt) // 4\n",
        "                output_tokens = len(response_text) // 4 if response_text else 0\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "        else:\n",
        "            error_message = f\"Model {model_name} not found in config for cost calculation.\"\n",
        "        result = {\"total_cost\": total_cost}\n",
        "        if error_message:\n",
        "            result[\"error\"] = error_message\n",
        "            print(f\"Cost calculation warning for {model_name}: {error_message}\")\n",
        "        return result\n",
        "\n",
        "    def query_llm(self, model_name, prompt):\n",
        "        raw_response_full, parsed_answer_tuple, cost_data = \"\", (\"\", \"\"), {}\n",
        "        usage_info = None\n",
        "        try:\n",
        "            self.linucb_model.respect_rate_limit(model_name)\n",
        "            if model_name in MODELS_CONFIG:\n",
        "                api_response = openrouter_client.chat.completions.create(\n",
        "                    model=model_name,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                )\n",
        "                raw_response_full = api_response.choices[0].message.content\n",
        "                if api_response.usage:\n",
        "                    usage_info = {\"prompt_tokens\": api_response.usage.prompt_tokens, \"completion_tokens\": api_response.usage.completion_tokens}\n",
        "                cost_data = self.calculate_token_cost(model_name, prompt, raw_response_full, usage_info=usage_info)\n",
        "                parsed_answer_tuple = self.parse_llm_answer(raw_response_full)\n",
        "                return raw_response_full, parsed_answer_tuple, cost_data\n",
        "            else:\n",
        "                raise ValueError(f\"Model {model_name} is not configured in MODELS_CONFIG.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying LLM {model_name}: {e}\")\n",
        "            self.linucb_model.register_model_call(model_name)\n",
        "            cost_data = self.calculate_token_cost(model_name, prompt, \"\", usage_info=None)\n",
        "            return \"\", (\"\", \"\"), cost_data\n",
        "\n",
        "    def run_cascade_single_question(self, question, current_question_budget=float('inf'), is_test_phase=False):\n",
        "        \"\"\"\n",
        "        Run the Value Density-based LinUCB cascade for a single question.\n",
        "        - question: The question to answer\n",
        "        - current_question_budget: Budget limit for this question (used in test phase)\n",
        "        - is_test_phase: Whether we're in the test phase (applies budget constraints)\n",
        "        \"\"\"\n",
        "        base_features = self.feature_extractor.extract_features(question)\n",
        "        failed_answers, failed_llm_ids, used_llms_this_question = [], [], []\n",
        "        question_total_cost, current_attempts_log = 0.0, []\n",
        "        final_status = \"Failure\"  # Default to failure, will update on success\n",
        "        remaining_budget = current_question_budget if is_test_phase else float('inf')\n",
        "\n",
        "        # Value Density Cascade Loop\n",
        "        for i in range(1, self.cascade_length + 1):\n",
        "            print(f\"Step {i}\")\n",
        "            # 1. Construct feature vector using history\n",
        "            x_i = self.feature_extractor.construct_feature_vector(base_features, i, failed_answers, failed_llm_ids, self.linucb_model.model_name_to_index)\n",
        "            # 2. Format prompt with context\n",
        "            prompt = self.format_prompt(question, failed_answers, failed_llm_ids)\n",
        "            # 3. Select model based on learned strategy, respecting budget\n",
        "            chosen_model, scores = self.linucb_model.select_model_value_density(x_i, i, prompt, remaining_budget, is_test_phase, REWARD_IMPORTANCE, COST_IMPORTANCE, used_llms_this_question)\n",
        "\n",
        "            if not chosen_model:\n",
        "                final_status = \"Failure - Budget Exceeded or No Suitable Model\" if is_test_phase else \"Failure - All LLMs Used (Train)\"\n",
        "                print(f\"Stopping cascade: {final_status}\")\n",
        "                break\n",
        "\n",
        "            print(f\"Selected: {chosen_model}, Strategy: {scores[chosen_model].get('selection_strategy', 'N/A')}, Score: {scores[chosen_model].get('selection_score', -inf):.4f}\")\n",
        "            # 4. Query the chosen model\n",
        "            raw_response, (answer_for_grading, explanation_text), cost_data = self.query_llm(chosen_model, prompt)\n",
        "            # 5. Extract costs\n",
        "            actual_total_cost = cost_data.get(\"total_cost\", 0.0)\n",
        "            actual_output_cost = cost_data.get(\"output_cost\", 0.0)\n",
        "            # 6. Update budget\n",
        "            question_total_cost += actual_total_cost\n",
        "            remaining_budget -= actual_total_cost\n",
        "            # 7. Update cost estimator with actual observed output cost\n",
        "            self.linucb_model.cost_estimator.update_stats(chosen_model, i, actual_output_cost)\n",
        "            # 8. Determine outcome using the grader\n",
        "            is_correct = self.grade_with_gemma12b(answer_for_grading, question['ground_truth_answer'])\n",
        "            reward = 1 if is_correct else 0\n",
        "            print(f\"Answer: {answer_for_grading}, Correct: {is_correct}, Cost: ${actual_total_cost:.8f}\")\n",
        "            # 9. Update LinUCB for the chosen model\n",
        "            self.linucb_model.update_reward_only(chosen_model, x_i, reward)\n",
        "            # 10. Log the attempt\n",
        "            current_attempts_log.append({\n",
        "                \"step\": i, \"chosen_model\": chosen_model, \"chosen_model_cost\": cost_data,\n",
        "                \"is_correct\": is_correct, \"reward_ri\": reward, \"llm_answer\": answer_for_grading,\n",
        "                \"llm_explanation\": explanation_text, \"scores_per_arm\": scores\n",
        "            })\n",
        "            # 11. Handle Outcome\n",
        "            if is_correct:\n",
        "                final_status = \"Success\"\n",
        "                print(f\"Success in step {i}!\")\n",
        "                break\n",
        "            else:\n",
        "                # Failure: Update history for the next step\n",
        "                failed_answers.append(answer_for_grading if answer_for_grading else \"Unknown\")\n",
        "                failed_llm_ids.append(chosen_model)\n",
        "                if chosen_model:\n",
        "                    used_llms_this_question.append(chosen_model)\n",
        "\n",
        "        return {\n",
        "            \"question\": question[\"question\"], \"ground_truth_answer\": question[\"ground_truth_answer\"],\n",
        "            \"final_status\": final_status, \"total_cost\": question_total_cost,\n",
        "            \"steps_taken\": len(current_attempts_log), \"attempts\": current_attempts_log,\n",
        "            \"is_test_phase\": is_test_phase, \"question_budget\": current_question_budget if is_test_phase else None\n",
        "        }\n",
        "\n",
        "\n",
        "def load_math500_dataset(json_path):\n",
        "    \"\"\"Load the Math500 dataset from a JSON file with robust error handling.\"\"\"\n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            raw_data = json.load(f)\n",
        "        # Assuming data is under the key \"test\"\n",
        "        return [{\n",
        "            \"question\": item[\"problem\"],\n",
        "            \"options\": [], # Math500 has no MCQs\n",
        "            \"ground_truth_answer\": item[\"answer\"],\n",
        "            \"unique_id\": item.get(\"unique_id\", \"Unknown\")\n",
        "        } for item in raw_data[\"test\"]]\n",
        "    except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:\n",
        "        print(f\"Error loading dataset from {json_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def save_records_with_backup(records, json_path):\n",
        "    if os.path.exists(json_path):\n",
        "        try:\n",
        "            os.replace(json_path, json_path + BACKUP_SUFFIX)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create backup: {e}\")\n",
        "    try:\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(records, f, indent=4)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving records: {e}\")\n",
        "        # Try to restore from backup if save failed\n",
        "        if os.path.exists(json_path + BACKUP_SUFFIX):\n",
        "            try:\n",
        "                os.replace(json_path + BACKUP_SUFFIX, json_path)\n",
        "            except: pass\n",
        "        return False\n",
        "\n",
        "def initialize_json_files():\n",
        "    if not os.path.exists(RECORDS_PATH):\n",
        "        with open(RECORDS_PATH, 'w') as f: json.dump([], f)\n",
        "    else:\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f:\n",
        "                if not isinstance(json.load(f), list): raise ValueError\n",
        "        except (json.JSONDecodeError, ValueError):\n",
        "            os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
        "            with open(RECORDS_PATH, 'w') as f: json.dump([], f)\n",
        "\n",
        "\n",
        "def analyze_results(records):\n",
        "    \"\"\"Analyze and print results from the records\"\"\"\n",
        "    if not records:\n",
        "        print(\"No records to analyze\")\n",
        "        return\n",
        "\n",
        "    # Separate records into training and test sets\n",
        "    train_records = [r for r in records if not r.get(\"is_test_phase\", False)]\n",
        "    test_records = [r for r in records if r.get(\"is_test_phase\", True)]\n",
        "\n",
        "    # Extract data for analysis\n",
        "    total_train_questions = len(train_records)\n",
        "    total_test_questions = len(test_records)\n",
        "\n",
        "    train_success = sum(1 for r in train_records if r[\"final_status\"] == \"Success\")\n",
        "    test_success = sum(1 for r in test_records if r[\"final_status\"] == \"Success\")\n",
        "\n",
        "    train_success_rate = train_success / total_train_questions if total_train_questions > 0 else 0\n",
        "    test_success_rate = test_success / total_test_questions if total_test_questions > 0 else 0\n",
        "\n",
        "    # Calculate total steps\n",
        "    total_train_steps = sum(r[\"steps_taken\"] for r in train_records)\n",
        "    total_test_steps = sum(r[\"steps_taken\"] for r in test_records)\n",
        "\n",
        "    # Calculate average steps\n",
        "    avg_train_steps = total_train_steps / total_train_questions if total_train_questions > 0 else 0\n",
        "    avg_test_steps = total_test_steps / total_test_questions if total_test_questions > 0 else 0\n",
        "\n",
        "    # Calculate success by position\n",
        "    train_successes_by_position = [0] * CASCADE_LENGTH\n",
        "    test_successes_by_position = [0] * CASCADE_LENGTH\n",
        "\n",
        "    for record in train_records:\n",
        "        if record[\"final_status\"] == \"Success\":\n",
        "            position = len(record[\"attempts\"]) - 1  # 0-indexed\n",
        "            if position < CASCADE_LENGTH:\n",
        "                train_successes_by_position[position] += 1\n",
        "\n",
        "    for record in test_records:\n",
        "        if record[\"final_status\"] == \"Success\":\n",
        "            position = len(record[\"attempts\"]) - 1  # 0-indexed\n",
        "            if position < CASCADE_LENGTH:\n",
        "                test_successes_by_position[position] += 1\n",
        "\n",
        "    train_per_position_success = [count/total_train_questions for count in train_successes_by_position] if total_train_questions > 0 else [0] * CASCADE_LENGTH\n",
        "    test_per_position_success = [count/total_test_questions for count in test_successes_by_position] if total_test_questions > 0 else [0] * CASCADE_LENGTH\n",
        "\n",
        "    # Calculate costs\n",
        "    train_total_cost = sum(r[\"total_cost\"] for r in train_records)\n",
        "    test_total_cost = sum(r[\"total_cost\"] for r in test_records)\n",
        "\n",
        "    train_avg_cost = train_total_cost / total_train_questions if total_train_questions > 0 else 0\n",
        "    test_avg_cost = test_total_cost / total_test_questions if total_test_questions > 0 else 0\n",
        "\n",
        "    # Calculate cost per successful question\n",
        "    train_success_cost = sum(r[\"total_cost\"] for r in train_records if r[\"final_status\"] == \"Success\")\n",
        "    test_success_cost = sum(r[\"total_cost\"] for r in test_records if r[\"final_status\"] == \"Success\")\n",
        "\n",
        "    train_avg_cost_success = train_success_cost / train_success if train_success > 0 else 0\n",
        "    test_avg_cost_success = test_success_cost / test_success if test_success > 0 else 0\n",
        "\n",
        "    # Count budget exceeded in test phase\n",
        "    budget_exceeded_count = sum(1 for r in test_records if r[\"final_status\"] == \"Failure - Budget Exceeded\")\n",
        "    budget_exceeded_rate = budget_exceeded_count / total_test_questions if total_test_questions > 0 else 0\n",
        "\n",
        "    # Analyze model performance with per-position breakdown\n",
        "    model_metrics = {\n",
        "        \"train\": {model: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0,\n",
        "                          \"by_position\": defaultdict(lambda: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0})}\n",
        "                 for model in AVAILABLE_LLMS},\n",
        "        \"test\": {model: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0,\n",
        "                         \"by_position\": defaultdict(lambda: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0})}\n",
        "                for model in AVAILABLE_LLMS}\n",
        "    }\n",
        "\n",
        "    # Track LCB cost estimation accuracy (output cost only)\n",
        "    lcb_cost_accuracy = {model: {\"total_lcb_estimate\": 0.0, \"total_actual_cost\": 0.0, \"count\": 0}\n",
        "                         for model in AVAILABLE_LLMS}\n",
        "\n",
        "    # Process training records\n",
        "    for record in train_records:\n",
        "        for attempt in record[\"attempts\"]:\n",
        "            model = attempt[\"chosen_model\"]\n",
        "            is_correct = attempt[\"is_correct\"]\n",
        "            cost = attempt[\"chosen_model_cost\"][\"total_cost\"]\n",
        "            position = attempt[\"step\"]\n",
        "\n",
        "            # Update train stats\n",
        "            model_metrics[\"train\"][model][\"calls\"] += 1\n",
        "            model_metrics[\"train\"][model][\"total_cost\"] += cost\n",
        "            model_metrics[\"train\"][model][\"by_position\"][position][\"calls\"] += 1\n",
        "            model_metrics[\"train\"][model][\"by_position\"][position][\"total_cost\"] += cost\n",
        "\n",
        "            if is_correct:\n",
        "                model_metrics[\"train\"][model][\"successes\"] += 1\n",
        "                model_metrics[\"train\"][model][\"by_position\"][position][\"successes\"] += 1\n",
        "\n",
        "    # Process test records\n",
        "    for record in test_records:\n",
        "        for attempt in record[\"attempts\"]:\n",
        "            model = attempt[\"chosen_model\"]\n",
        "            is_correct = attempt[\"is_correct\"]\n",
        "            cost = attempt[\"chosen_model_cost\"][\"total_cost\"]\n",
        "            position = attempt[\"step\"]\n",
        "\n",
        "            # Update test stats\n",
        "            model_metrics[\"test\"][model][\"calls\"] += 1\n",
        "            model_metrics[\"test\"][model][\"total_cost\"] += cost\n",
        "            model_metrics[\"test\"][model][\"by_position\"][position][\"calls\"] += 1\n",
        "            model_metrics[\"test\"][model][\"by_position\"][position][\"total_cost\"] += cost\n",
        "\n",
        "            if is_correct:\n",
        "                model_metrics[\"test\"][model][\"successes\"] += 1\n",
        "                model_metrics[\"test\"][model][\"by_position\"][position][\"successes\"] += 1\n",
        "\n",
        "            # Track LCB output cost estimation accuracy (test phase only)\n",
        "            if \"chosen_model_cost\" in attempt and \"scores_per_arm\" in attempt and model in attempt[\"scores_per_arm\"]:\n",
        "                actual_output_cost = attempt[\"chosen_model_cost\"][\"output_cost\"]\n",
        "                if \"lcb_output_cost_estimate\" in attempt[\"scores_per_arm\"][model]:\n",
        "                    lcb_cost_accuracy[model][\"total_lcb_estimate\"] += attempt[\"scores_per_arm\"][model][\"lcb_output_cost_estimate\"]\n",
        "                    lcb_cost_accuracy[model][\"total_actual_cost\"] += actual_output_cost\n",
        "                    lcb_cost_accuracy[model][\"count\"] += 1\n",
        "\n",
        "    # Calculate success rates and average costs for models\n",
        "    for phase in [\"train\", \"test\"]:\n",
        "        for model, data in model_metrics[phase].items():\n",
        "            if data[\"calls\"] > 0:\n",
        "                data[\"success_rate\"] = data[\"successes\"] / data[\"calls\"]\n",
        "                data[\"avg_cost\"] = data[\"total_cost\"] / data[\"calls\"]\n",
        "\n",
        "                # Calculate position-specific metrics\n",
        "                for pos, pos_data in data[\"by_position\"].items():\n",
        "                    if pos_data[\"calls\"] > 0:\n",
        "                        pos_data[\"success_rate\"] = pos_data[\"successes\"] / pos_data[\"calls\"]\n",
        "                        pos_data[\"avg_cost\"] = pos_data[\"total_cost\"] / pos_data[\"calls\"]\n",
        "                    else:\n",
        "                        pos_data[\"success_rate\"] = 0\n",
        "                        pos_data[\"avg_cost\"] = 0\n",
        "            else:\n",
        "                data[\"success_rate\"] = 0\n",
        "                data[\"avg_cost\"] = 0\n",
        "\n",
        "    # Calculate LCB estimation accuracy\n",
        "    for model, data in lcb_cost_accuracy.items():\n",
        "        if data[\"count\"] > 0:\n",
        "            data[\"avg_lcb_estimate\"] = data[\"total_lcb_estimate\"] / data[\"count\"]\n",
        "            data[\"avg_actual_cost\"] = data[\"total_actual_cost\"] / data[\"count\"]\n",
        "            data[\"lcb_accuracy\"] = data[\"avg_lcb_estimate\"] / data[\"avg_actual_cost\"] if data[\"avg_actual_cost\"] > 0 else 0\n",
        "        else:\n",
        "            data[\"avg_lcb_estimate\"] = 0\n",
        "            data[\"avg_actual_cost\"] = 0\n",
        "            data[\"lcb_accuracy\"] = 0\n",
        "\n",
        "    # Generate summary text\n",
        "    summary = \"=== BUDGET-AWARE LinUCB CASCADE (MATH LATEX RESPONSE) ===\\n\\n\"\n",
        "    summary += f\"LLM Output Cost Beta: {BETA_COST}\\n\\n\"\n",
        "\n",
        "    summary += \"=== TRAIN PHASE RESULTS (30%) ===\\n\"\n",
        "    summary += f\"Total Questions: {total_train_questions}\\n\"\n",
        "    summary += f\"Success Rate: {train_success_rate:.4f}\\n\"\n",
        "    summary += f\"Average Steps (Train): {avg_train_steps:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${train_avg_cost:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${train_avg_cost_success:.8f}\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(train_per_position_success):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== TEST PHASE RESULTS (70%) ===\\n\"\n",
        "    summary += f\"Total Questions: {total_test_questions}\\n\"\n",
        "    summary += f\"Success Rate: {test_success_rate:.4f}\\n\"\n",
        "    summary += f\"Average Steps (Test): {avg_test_steps:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${test_avg_cost:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${test_avg_cost_success:.8f}\\n\"\n",
        "    summary += f\"Budget Exceeded Count: {budget_exceeded_count} ({budget_exceeded_rate:.4f})\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(test_per_position_success):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== MODEL PERFORMANCE (TRAIN PHASE) ===\\n\"\n",
        "    for model, metrics in model_metrics[\"train\"].items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "\n",
        "            # No need for position breakdown in train phase summary\n",
        "\n",
        "    summary += \"\\n=== MODEL PERFORMANCE (TEST PHASE) ===\\n\"\n",
        "    for model, metrics in model_metrics[\"test\"].items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "            summary += f\"  By Position:\\n\"\n",
        "\n",
        "            # Add per-position breakdown\n",
        "            for pos in sorted(metrics[\"by_position\"].keys()):\n",
        "                pos_data = metrics[\"by_position\"][pos]\n",
        "                if pos_data[\"calls\"] > 0:\n",
        "                    summary += f\"    Pos {pos}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, \"\n",
        "                    summary += f\"Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== LLM LCB OUTPUT COST ESTIMATION ACCURACY (TEST PHASE) ===\\n\"\n",
        "    for model, data in lcb_cost_accuracy.items():\n",
        "        if data[\"count\"] > 0:\n",
        "            summary += f\"{model}: Avg LCB Est: ${data['avg_lcb_estimate']:.8f}, \"\n",
        "            summary += f\"Avg Actual: ${data['avg_actual_cost']:.8f}, Ratio: {data['lcb_accuracy']:.4f}\\n\"\n",
        "\n",
        "    summary += f\"\\nTotal Overall Cost (Train): ${train_total_cost:.8f}\\n\"\n",
        "    summary += f\"Total Overall Cost (Test): ${test_total_cost:.8f}\\n\"\n",
        "    summary += f\"Total Overall Cost: ${(train_total_cost + test_total_cost):.8f}\\n\"\n",
        "\n",
        "    # Print and save summary\n",
        "    print(summary)\n",
        "\n",
        "    with open(SUMMARY_STATS_PATH, 'w') as f:\n",
        "        f.write(summary)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Starting LinUCB Cascade with Value Density & Budget (LaTeX Math Response)\")\n",
        "    initialize_json_files()\n",
        "    dataset_full = load_math500_dataset(INPUT_JSON)\n",
        "    if not dataset_full:\n",
        "        print(\"Dataset is empty or could not be loaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        num_to_process = int(input(f\"How many questions to process? (1-{len(dataset_full)}): \"))\n",
        "        dataset = random.sample(dataset_full, min(num_to_process, len(dataset_full)))\n",
        "    except ValueError:\n",
        "        dataset = random.sample(dataset_full, 10)\n",
        "    print(f\"Using {len(dataset)} questions from the Math500 dataset\")\n",
        "\n",
        "    feature_extractor = FeatureExtractor()\n",
        "    feature_extractor.initialize(dataset)\n",
        "    linucb_model = LinUCBModel(model_names=AVAILABLE_LLMS)\n",
        "    cascade = BatchBudgetCascade(feature_extractor, linucb_model)\n",
        "\n",
        "    # Load existing records to determine starting point\n",
        "    all_records = []\n",
        "    if os.path.exists(RECORDS_PATH):\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f: all_records = json.load(f)\n",
        "            processed_questions = {r[\"question\"] for r in all_records}\n",
        "            dataset = [q for q in dataset if q[\"question\"] not in processed_questions]\n",
        "            print(f\"Found {len(all_records)} existing records, {len(dataset)} questions remaining\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing records: {e}\")\n",
        "\n",
        "    if os.path.exists(LINUCB_MODEL_PATH):\n",
        "        linucb_model.load_model_state(LINUCB_MODEL_PATH)\n",
        "\n",
        "    train_size = int(0.2 * len(dataset))\n",
        "    print(f\"Train size: {train_size}, Test size: {len(dataset) - train_size}\")\n",
        "    training_total_cost, training_question_count, avg_train_cost = 0.0, 0, 0.0\n",
        "\n",
        "    try:\n",
        "        for idx, question in enumerate(dataset):\n",
        "            is_test_phase = idx >= train_size\n",
        "            # Calculate budget (only in test phase)\n",
        "            current_question_budget = float('inf')\n",
        "            if is_test_phase:\n",
        "                # Use average cost from training phase as the budget for test questions\n",
        "                if training_question_count > 0 and avg_train_cost == 0.0:\n",
        "                    avg_train_cost = training_total_cost / training_question_count\n",
        "                current_question_budget = 0.00014217 # Fixed budget for reproducibility\n",
        "                print(f\"Test phase budget: ${current_question_budget:.8f}\")\n",
        "\n",
        "            print(f\"\\nProcessing question {idx+1}/{len(dataset)} ({'Test' if is_test_phase else 'Train'}): {question['question'][:80]}...\")\n",
        "            question_record = cascade.run_cascade_single_question(question, current_question_budget, is_test_phase)\n",
        "            all_records.append(question_record)\n",
        "\n",
        "            if not is_test_phase:\n",
        "                training_question_count += 1\n",
        "                training_total_cost += question_record[\"total_cost\"]\n",
        "\n",
        "            # Save records and states periodically\n",
        "            if (idx + 1) % UPDATE_FREQUENCY == 0 or idx == train_size - 1:\n",
        "                save_records_with_backup(all_records, RECORDS_PATH)\n",
        "                linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProcessing interrupted. Saving progress...\")\n",
        "    finally:\n",
        "        save_records_with_backup(all_records, RECORDS_PATH)\n",
        "        linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "        analyze_results(all_records)\n",
        "        print(\"LinUCB Cascade completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}