{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import tiktoken\n",
        "import google.generativeai as genai\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_FEATURE_DIM = 384  # Dimension for question embeddings\n",
        "ANSWER_EMBED_DIM = 384  # Dimension for answer embeddings\n",
        "CASCADE_LENGTH = 4      # Number of attempts in the cascade (K)\n",
        "UPDATE_FREQUENCY = 1    # Update JSON records after every question\n",
        "USE_EMBEDDINGS = True   # Use embeddings instead of TF-IDF\n",
        "\n",
        "# LinUCB parameters\n",
        "ALPHA = 0.675           # Exploration parameter for LinUCB\n",
        "LAMBDA_REG = 0.45       # Regularization parameter for LinUCB matrix initialization\n",
        "\n",
        "# Value Density & Cost Estimation parameters\n",
        "BETA_COST = 1.0         # Controls width of LCB interval for LLM cost estimation\n",
        "EPSILON_VD = 1e-10      # Small positive value for value density calculation\n",
        "DEFAULT_FALLBACK_COST = 0.01  # Default cost when insufficient data\n",
        "\n",
        "# Question Cost Estimation parameters\n",
        "BETA_COST_QUESTION = 1.0  # Controls width of LCB interval for question cost estimation\n",
        "DEFAULT_FALLBACK_COST_QUESTION = 0.04  # Default cost for questions when insufficient data\n",
        "QUESTION_POOL_MULTIPLIER = 3  # How many times BATCH_SIZE to consider for question selection\n",
        "\n",
        "# Pruning configuration\n",
        "MAX_WORDS = 250\n",
        "REPETITION_THRESHOLD = 3\n",
        "\n",
        "# Batch parameters\n",
        "BATCH_SIZE = 17"
      ],
      "metadata": {
        "id": "LgB_MxmPkCJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENROUTER_API_KEY =  # Replace with your Openrouterkey\n",
        "OPENROUTER_BASE_URL =  # Replace with your openrouter URL"
      ],
      "metadata": {
        "id": "j3E-rqUtjhSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_CONFIG = {\n",
        "    \"mistralai/mistral-small-3.1-24b-instruct\": {\"input_cost\": 0.05 / 1e6, \"output_cost\": 0.15 / 1e6},\n",
        "    \"microsoft/phi-4\": {\"input_cost\": 0.07 / 1e6, \"output_cost\": 0.14 / 1e6},\n",
        "    \"meta-llama/llama-4-maverick\": {\"input_cost\": 0.17 / 1e6, \"output_cost\": 0.16 / 1e6},\n",
        "    \"google/gemini-2.0-flash-001\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"openai/gpt-4.1-nano\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"deepseek/deepseek-chat\": {\"input_cost\": 0.38 / 1e6, \"output_cost\": 0.89 / 1e6},\n",
        "}\n",
        "\n",
        "GRADER_MODEL_NAME = \"google/gemini-2.0-flash-lite-001\"\n",
        "\n",
        "AVAILABLE_LLMS = list(MODELS_CONFIG.keys())\n",
        "LLM_ID_DIM = len(AVAILABLE_LLMS)\n",
        "\n",
        "# Feature dimensions are recalculated based on the number of available LLMs\n",
        "CONTEXT_FEATURE_DIM = ANSWER_EMBED_DIM + LLM_ID_DIM + ANSWER_EMBED_DIM\n",
        "TOTAL_FEATURE_DIM = BASE_FEATURE_DIM + 1 + CONTEXT_FEATURE_DIM\n",
        "\n",
        "# Initialize OpenRouter client\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=OPENROUTER_BASE_URL,\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        ")\n",
        "\n",
        "# File paths\n",
        "INPUT_JSON = \"Math500.json\"\n",
        "RECORDS_PATH = \"M93.json\"\n",
        "LINUCB_MODEL_PATH = \"M93.npz\"\n",
        "SUMMARY_STATS_PATH = \"M93.txt\"\n",
        "QUESTION_COST_PATH = \"Question_Cost_Estimator.npz\"\n",
        "BACKUP_SUFFIX = \".bak\"\n",
        "\n",
        "def solve_0_1_knapsack(items, capacity):\n",
        "    \"\"\"\n",
        "    Solves the 0/1 knapsack problem to find the optimal subset of items.\n",
        "    This is used to select a value-maximizing subset of LLMs that fits within a budget.\n",
        "    - items: List of dictionaries, each with 'name', 'value' (UCB score), and 'weight' (LCB cost).\n",
        "    - capacity: Maximum total weight (budget) the knapsack can hold.\n",
        "    \"\"\"\n",
        "    if not items or capacity <= 0:\n",
        "        return []\n",
        "    n = len(items)\n",
        "    # Scale weights by 10000 to handle floating point weights as integers for DP table\n",
        "    scaled_capacity = int(capacity * 10000)\n",
        "    scaled_weights = [int(item['weight'] * 10000) for item in items]\n",
        "    dp = [[0 for _ in range(scaled_capacity + 1)] for _ in range(n + 1)]\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        for w in range(scaled_capacity + 1):\n",
        "            if scaled_weights[i-1] <= w:\n",
        "                dp[i][w] = max(dp[i-1][w], items[i-1]['value'] + dp[i-1][w - scaled_weights[i-1]])\n",
        "            else:\n",
        "                dp[i][w] = dp[i-1][w]\n",
        "\n",
        "    # Backtrack to find which items were selected\n",
        "    selected_items = []\n",
        "    w = scaled_capacity\n",
        "    for i in range(n, 0, -1):\n",
        "        if dp[i][w] != dp[i-1][w]:\n",
        "            selected_items.append(items[i-1]['name'])\n",
        "            w -= scaled_weights[i-1]\n",
        "    return selected_items\n",
        "\n",
        "\n",
        "class QuestionCostEstimator:\n",
        "    def __init__(self):\n",
        "        self.stats = {\"count\": 0, \"total_cost\": 0.0, \"sum_sq_cost\": 0.0}\n",
        "\n",
        "    def update_stats(self, actual_question_cost):\n",
        "        self.stats[\"count\"] += 1\n",
        "        self.stats[\"total_cost\"] += actual_question_cost\n",
        "        self.stats[\"sum_sq_cost\"] += actual_question_cost ** 2\n",
        "\n",
        "    def estimate_question_lcb_cost(self, beta=BETA_COST_QUESTION, default_cost=DEFAULT_FALLBACK_COST_QUESTION):\n",
        "        \"\"\"Estimate the Lower Confidence Bound (LCB) of cost for a question.\"\"\"\n",
        "        if self.stats[\"count\"] >= 2:\n",
        "            mean_cost = self.stats[\"total_cost\"] / self.stats[\"count\"]\n",
        "            variance = (self.stats[\"sum_sq_cost\"] / self.stats[\"count\"]) - (mean_cost ** 2)\n",
        "            std_dev = math.sqrt(max(0, variance))\n",
        "            std_error = std_dev / math.sqrt(self.stats[\"count\"])\n",
        "            lcb_cost = mean_cost - beta * std_error\n",
        "            return max(EPSILON_VD / 10, lcb_cost)\n",
        "        return default_cost\n",
        "\n",
        "    def save_state(self, file_path):\n",
        "        try:\n",
        "            np.savez_compressed(file_path, **self.stats)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving question cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_state(self, file_path):\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            self.stats[\"count\"] = int(loaded[\"count\"])\n",
        "            self.stats[\"total_cost\"] = float(loaded[\"total_cost\"])\n",
        "            self.stats[\"sum_sq_cost\"] = float(loaded[\"sum_sq_cost\"])\n",
        "            print(f\"Loaded question cost estimator state from {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading question cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class ModelCostEstimator:\n",
        "    def __init__(self, model_names):\n",
        "        self.stats = {}\n",
        "        for model_name in model_names:\n",
        "            self.stats[model_name] = {\n",
        "                \"overall\": {\"count\": 0, \"total_cost\": 0.0, \"sum_sq_cost\": 0.0},\n",
        "                \"by_position\": {\n",
        "                    str(i): {\"count\": 0, \"total_cost\": 0.0, \"sum_sq_cost\": 0.0}\n",
        "                    for i in range(1, CASCADE_LENGTH + 1)\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def update_stats(self, model_name, step, actual_cost):\n",
        "        step_str = str(step)\n",
        "        if model_name in self.stats:\n",
        "            # Update position-specific stats\n",
        "            if step_str in self.stats[model_name][\"by_position\"]:\n",
        "                pos_stats = self.stats[model_name][\"by_position\"][step_str]\n",
        "                pos_stats[\"count\"] += 1\n",
        "                pos_stats[\"total_cost\"] += actual_cost\n",
        "                pos_stats[\"sum_sq_cost\"] += actual_cost ** 2\n",
        "            # Update overall stats\n",
        "            overall_stats = self.stats[model_name][\"overall\"]\n",
        "            overall_stats[\"count\"] += 1\n",
        "            overall_stats[\"total_cost\"] += actual_cost\n",
        "            overall_stats[\"sum_sq_cost\"] += actual_cost ** 2\n",
        "\n",
        "    def estimate_lcb_cost(self, model_name, step, beta=BETA_COST, default_cost=DEFAULT_FALLBACK_COST):\n",
        "        \"\"\"Estimate the Lower Confidence Bound (LCB) of cost for a model at a specific step.\"\"\"\n",
        "        step_str = str(step)\n",
        "        # Try position-specific stats first\n",
        "        if model_name in self.stats and step_str in self.stats[model_name][\"by_position\"]:\n",
        "            pos_stats = self.stats[model_name][\"by_position\"][step_str]\n",
        "            if pos_stats[\"count\"] >= 2:\n",
        "                mean_cost = pos_stats[\"total_cost\"] / pos_stats[\"count\"]\n",
        "                variance = (pos_stats[\"sum_sq_cost\"] / pos_stats[\"count\"]) - (mean_cost ** 2)\n",
        "                std_dev = math.sqrt(max(0, variance))\n",
        "                std_error = std_dev / math.sqrt(pos_stats[\"count\"])\n",
        "                return max(EPSILON_VD / 10, mean_cost - beta * std_error)\n",
        "        # Fallback to overall stats\n",
        "        if model_name in self.stats:\n",
        "            overall_stats = self.stats[model_name][\"overall\"]\n",
        "            if overall_stats[\"count\"] >= 2:\n",
        "                mean_cost = overall_stats[\"total_cost\"] / overall_stats[\"count\"]\n",
        "                variance = (overall_stats[\"sum_sq_cost\"] / overall_stats[\"count\"]) - (mean_cost ** 2)\n",
        "                std_dev = math.sqrt(max(0, variance))\n",
        "                std_error = std_dev / math.sqrt(overall_stats[\"count\"])\n",
        "                return max(EPSILON_VD / 10, mean_cost - beta * std_error)\n",
        "        return default_cost\n",
        "\n",
        "    def save_state(self, file_path):\n",
        "        try:\n",
        "            save_dict = {}\n",
        "            for model_name, model_stats in self.stats.items():\n",
        "                overall = model_stats[\"overall\"]\n",
        "                save_dict[f\"overall_count_{model_name}\"] = overall[\"count\"]\n",
        "                save_dict[f\"overall_total_cost_{model_name}\"] = overall[\"total_cost\"]\n",
        "                save_dict[f\"overall_sum_sq_cost_{model_name}\"] = overall[\"sum_sq_cost\"]\n",
        "                for pos, pos_stats in model_stats[\"by_position\"].items():\n",
        "                    save_dict[f\"pos{pos}_count_{model_name}\"] = pos_stats[\"count\"]\n",
        "                    save_dict[f\"pos{pos}_total_cost_{model_name}\"] = pos_stats[\"total_cost\"]\n",
        "                    save_dict[f\"pos{pos}_sum_sq_cost_{model_name}\"] = pos_stats[\"sum_sq_cost\"]\n",
        "            np.savez_compressed(file_path, **save_dict)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_state(self, file_path):\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            for model_name in self.stats:\n",
        "                if f\"overall_count_{model_name}\" in loaded:\n",
        "                    overall = self.stats[model_name][\"overall\"]\n",
        "                    overall[\"count\"] = int(loaded[f\"overall_count_{model_name}\"])\n",
        "                    overall[\"total_cost\"] = float(loaded[f\"overall_total_cost_{model_name}\"])\n",
        "                    overall[\"sum_sq_cost\"] = float(loaded[f\"overall_sum_sq_cost_{model_name}\"])\n",
        "                for pos in self.stats[model_name][\"by_position\"]:\n",
        "                    if f\"pos{pos}_count_{model_name}\" in loaded:\n",
        "                        pos_stats = self.stats[model_name][\"by_position\"][pos]\n",
        "                        pos_stats[\"count\"] = int(loaded[f\"pos{pos}_count_{model_name}\"])\n",
        "                        pos_stats[\"total_cost\"] = float(loaded[f\"pos{pos}_total_cost_{model_name}\"])\n",
        "                        pos_stats[\"sum_sq_cost\"] = float(loaded[f\"pos{pos}_sum_sq_cost_{model_name}\"])\n",
        "            print(f\"Loaded model cost estimator state from {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, feature_dim=BASE_FEATURE_DIM, use_embeddings=USE_EMBEDDINGS):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.use_embeddings = use_embeddings\n",
        "        if use_embeddings:\n",
        "            try:\n",
        "                self.embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
        "                print(\"Initialized sentence transformer embedding model\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error initializing sentence transformer: {e}\")\n",
        "                print(\"Falling back to TF-IDF vectorization\")\n",
        "                self.use_embeddings = False\n",
        "        if not self.use_embeddings:\n",
        "            # Initialize TF-IDF vectorizer without restricting features yet\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            # We'll use SVD for dimensionality reduction later\n",
        "            self.svd = None\n",
        "\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, questions):\n",
        "        \"\"\"Initialize the vectorizer with the corpus of questions\"\"\"\n",
        "        if not self.use_embeddings:\n",
        "            # Combine question text and options for each question\n",
        "            all_text = [q[\"problem\"] for q in questions] # Use \"problem\" field, no \"options\"\n",
        "\n",
        "\n",
        "            # Fit the vectorizer on all training texts\n",
        "            self.vectorizer.fit(all_text)\n",
        "\n",
        "            # Get the document-term matrix for the entire corpus\n",
        "            dtm = self.vectorizer.transform(all_text)\n",
        "\n",
        "            # Use SVD for dimensionality reduction to get feature_dim dense features\n",
        "            from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "            # Determine the appropriate dimension to reduce to (min of feature_dim and actual feature count)\n",
        "            n_components = min(self.feature_dim, dtm.shape[1])\n",
        "\n",
        "            self.svd = TruncatedSVD(n_components=n_components)\n",
        "            self.svd.fit(dtm)\n",
        "\n",
        "            print(f\"Using TF-IDF with SVD dimensionality reduction to {n_components} features\")\n",
        "            print(f\"Explained variance ratio: {sum(self.svd.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "    def extract_features(self, question):\n",
        "        \"\"\"Extract features from a question and its options\"\"\"\n",
        "        if not self.initialized:\n",
        "            raise ValueError(\"Feature extractor not initialized. Call initialize() first.\")\n",
        "\n",
        "        text = question[\"problem\"] # Use \"problem\" field, no \"options\"\n",
        "\n",
        "        if self.use_embeddings:\n",
        "            # Use sentence transformer for embeddings\n",
        "            features = self.embedding_model.encode([text])[0]\n",
        "        else:\n",
        "            # First get TF-IDF vector\n",
        "            tfidf_vector = self.vectorizer.transform([text])\n",
        "            # Then apply SVD transformation\n",
        "            features = self.svd.transform(tfidf_vector)[0]\n",
        "            # Ensure we have exactly feature_dim dimensions\n",
        "            if len(features) < self.feature_dim:\n",
        "                padding = np.zeros(self.feature_dim - len(features))\n",
        "                features = np.concatenate([features, padding])\n",
        "        return features\n",
        "\n",
        "    def extract_answer_features(self, answer_text):\n",
        "        \"\"\"Extract features from an answer string\"\"\"\n",
        "        if not answer_text:\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "        if self.use_embeddings:\n",
        "            try:\n",
        "                # Use sentence transformer for embeddings\n",
        "                features = self.embedding_model.encode([answer_text])[0]\n",
        "\n",
        "                # Handle if the embedding doesn't match the expected dimension\n",
        "                if len(features) != ANSWER_EMBED_DIM:\n",
        "                    if len(features) > ANSWER_EMBED_DIM:\n",
        "                        features = features[:ANSWER_EMBED_DIM]\n",
        "                    else:\n",
        "                        padding = np.zeros(ANSWER_EMBED_DIM - len(features))\n",
        "                        features = np.concatenate([features, padding])\n",
        "\n",
        "                return features\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding answer: {e}\")\n",
        "                return np.zeros(ANSWER_EMBED_DIM)\n",
        "        else:\n",
        "            # For simplicity, use zero vector if not using embeddings\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "    def construct_feature_vector(self, base_features, step_i, failed_answers, failed_llm_ids, model_name_to_index):\n",
        "        \"\"\"\n",
        "        Construct the augmented feature vector for LinUCB\n",
        "\n",
        "        Parameters:\n",
        "        - base_features: Features extracted from the question\n",
        "        - step_i: Current step in the cascade (1-indexed)\n",
        "        - failed_answers: List of previous failed answer strings\n",
        "        - failed_llm_ids: List of previous failed LLM IDs\n",
        "        - model_name_to_index: Mapping from model names to indices\n",
        "\n",
        "        Returns:\n",
        "        - Augmented feature vector\n",
        "        \"\"\"\n",
        "        # Normalize the step (1-indexed -> [0,1] range)\n",
        "        normalized_step = np.array([step_i / CASCADE_LENGTH])\n",
        "\n",
        "        # Initialize context features - last answer embedding\n",
        "        if step_i == 1 or not failed_answers:\n",
        "            # No prior context for the first step\n",
        "            last_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        else:\n",
        "            # Extract features from the most recent failed answer\n",
        "            last_answer = failed_answers[-1]\n",
        "            last_answer_features = self.extract_answer_features(last_answer)\n",
        "\n",
        "        # Calculate Last Failed LLM ID One-Hot encoding\n",
        "        last_llm_onehot = np.zeros(LLM_ID_DIM)\n",
        "        if step_i > 1 and failed_llm_ids:\n",
        "            last_llm_name = failed_llm_ids[-1]\n",
        "            if last_llm_name in model_name_to_index:\n",
        "                last_llm_index = model_name_to_index[last_llm_name]\n",
        "                last_llm_onehot[last_llm_index] = 1.0\n",
        "\n",
        "        # Calculate Average Failed Answer Embedding\n",
        "        avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        if step_i > 1 and failed_answers:\n",
        "            all_answer_features = [self.extract_answer_features(ans) for ans in failed_answers]\n",
        "            if all_answer_features:\n",
        "                avg_answer_features = np.mean(all_answer_features, axis=0)\n",
        "\n",
        "        # Handle case where avg_answer_features might be a scalar\n",
        "        if avg_answer_features.shape == ():\n",
        "            avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "        # Concatenate all context features\n",
        "        context_features = np.concatenate([\n",
        "            last_answer_features,    # Dim: ANSWER_EMBED_DIM\n",
        "            last_llm_onehot,         # Dim: LLM_ID_DIM\n",
        "            avg_answer_features      # Dim: ANSWER_EMBED_DIM\n",
        "        ])\n",
        "\n",
        "        # Final feature vector\n",
        "        augmented_features = np.concatenate([\n",
        "            base_features,           # Dim: BASE_FEATURE_DIM\n",
        "            normalized_step,         # Dim: 1\n",
        "            context_features         # Dim: CONTEXT_FEATURE_DIM\n",
        "        ])\n",
        "\n",
        "        # Ensure the final dimension matches expectations\n",
        "        if augmented_features.shape[0] != TOTAL_FEATURE_DIM:\n",
        "            # This should not happen if dimensions are calculated correctly\n",
        "            raise ValueError(f\"Constructed feature vector dimension {augmented_features.shape[0]} != expected {TOTAL_FEATURE_DIM}\")\n",
        "\n",
        "        return augmented_features\n",
        "\n",
        "\n",
        "class LinUCBModel:\n",
        "    def __init__(self, model_names, feature_dim=TOTAL_FEATURE_DIM, alpha=ALPHA,\n",
        "                 lambda_reg=LAMBDA_REG, beta_cost=BETA_COST,\n",
        "                 default_fallback_cost=DEFAULT_FALLBACK_COST):\n",
        "        self.model_names = model_names\n",
        "        self.feature_dim = feature_dim\n",
        "        self.alpha = alpha\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.beta_cost = beta_cost\n",
        "        self.default_fallback_cost = default_fallback_cost\n",
        "\n",
        "        self.model_name_to_index = {name: i for i, name in enumerate(model_names)}\n",
        "\n",
        "        # Initialize cost estimator\n",
        "        self.cost_estimator = ModelCostEstimator(model_names)\n",
        "\n",
        "        # Initialize model parameters\n",
        "        self.models = {}\n",
        "        for model_name in model_names:\n",
        "            self.models[model_name] = {\n",
        "                'A': np.identity(feature_dim) * lambda_reg,\n",
        "                'b': np.zeros(feature_dim),\n",
        "                'last_call_time': 0\n",
        "            }\n",
        "\n",
        "    def update_reward_only(self, model_name, feature_vector, reward):\n",
        "        \"\"\"\n",
        "        Update the LinUCB model parameters based on observed reward\n",
        "        Parameters:\n",
        "        - model_name: Name of the model to update\n",
        "        - feature_vector: Feature vector used for selection\n",
        "        - reward: Binary reward (1 for success, 0 for failure)\n",
        "        \"\"\"\n",
        "        model = self.models[model_name]\n",
        "        # Update A matrix\n",
        "        model['A'] += np.outer(feature_vector, feature_vector)\n",
        "        # Update b vector\n",
        "        model['b'] += feature_vector * reward\n",
        "\n",
        "    def get_predicted_reward(self, model_name, feature_vector):\n",
        "        \"\"\"\n",
        "        Get just the predicted reward (p_ia) for a given model and feature vector\n",
        "\n",
        "        Parameters:\n",
        "        - model_name: Name of the model\n",
        "        - feature_vector: Feature vector for prediction\n",
        "\n",
        "        Returns:\n",
        "        - Predicted reward (p_ia)\n",
        "        \"\"\"\n",
        "        model = self.models[model_name]\n",
        "        try:\n",
        "            # Calculate theta using the model's A matrix and b vector\n",
        "            theta = np.linalg.solve(model['A'], model['b'])\n",
        "            # Calculate predicted reward\n",
        "            p_ia = feature_vector.dot(theta)\n",
        "            return float(p_ia)\n",
        "        except np.linalg.LinAlgError:\n",
        "            try:\n",
        "                # Alternative way to calculate theta if direct solve fails\n",
        "                A_inv = np.linalg.inv(model['A'])\n",
        "                theta = A_inv.dot(model['b'])\n",
        "                p_ia = feature_vector.dot(theta)\n",
        "                return float(p_ia)\n",
        "            except:\n",
        "                # Return 0.0 if calculation completely fails\n",
        "                return 0.0\n",
        "\n",
        "    def calculate_ucb_scores(self, feature_vector):\n",
        "        \"\"\"\n",
        "        Calculate UCB scores for model selection\n",
        "\n",
        "        Parameters:\n",
        "        - feature_vector: Feature vector used for model selection\n",
        "\n",
        "        Returns a dictionary with scores for each model\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        for model_name in self.model_names:\n",
        "            model = self.models[model_name]\n",
        "            try:\n",
        "                L = np.linalg.cholesky(model['A'])\n",
        "                theta = np.linalg.solve(model['A'], model['b'])\n",
        "                z = np.linalg.solve(L, feature_vector)\n",
        "                ucb_term = self.alpha * np.sqrt(np.sum(z**2))\n",
        "                expected_reward = feature_vector.dot(theta)\n",
        "                ucb_score = expected_reward + ucb_term\n",
        "                scores[model_name] = {\n",
        "                    \"p_ia\": float(expected_reward),\n",
        "                    \"e_ia\": float(ucb_term),\n",
        "                    \"ucb_score\": float(ucb_score)\n",
        "                }\n",
        "            except np.linalg.LinAlgError:\n",
        "                try:\n",
        "                    A_inv = np.linalg.inv(model['A'])\n",
        "                    theta = A_inv.dot(model['b'])\n",
        "                    ucb_term = self.alpha * np.sqrt(feature_vector.dot(A_inv).dot(feature_vector))\n",
        "                    expected_reward = feature_vector.dot(theta)\n",
        "                    ucb_score = expected_reward + ucb_term\n",
        "                    scores[model_name] = {\n",
        "                        \"p_ia\": float(expected_reward),\n",
        "                        \"e_ia\": float(ucb_term),\n",
        "                        \"ucb_score\": float(ucb_score)\n",
        "                    }\n",
        "                except:\n",
        "                    scores[model_name] = {\n",
        "                        \"p_ia\": 0.0,\n",
        "                        \"e_ia\": 0.0,\n",
        "                        \"ucb_score\": 0.0\n",
        "                    }\n",
        "        return scores\n",
        "\n",
        "    def register_model_call(self, model_name):\n",
        "        \"\"\"Register that a model was called and update its last call time\"\"\"\n",
        "        self.models[model_name]['last_call_time'] = time.time()\n",
        "    def respect_rate_limit(self, model_name):\n",
        "        \"\"\"Wait if necessary to respect the model's rate limit.\"\"\"\n",
        "        # MODELS_CONFIG now refers to the OpenRouter models config, which doesn't have 'rpm'\n",
        "        model_cfg = MODELS_CONFIG.get(model_name)\n",
        "\n",
        "        # Only apply client-side RPM-based waiting if 'rpm' is defined for the model.\n",
        "        # For the specified OpenRouter models, 'rpm' is not defined, so this block will be skipped.\n",
        "        if model_cfg and \"rpm\" in model_cfg:\n",
        "            model_state = self.models[model_name] # 'model' is a local var, 'self.models' holds states\n",
        "            rpm = model_cfg[\"rpm\"]\n",
        "\n",
        "            min_seconds_between_calls = 60.0 / rpm\n",
        "\n",
        "            time_since_last_call = time.time() - model_state['last_call_time']\n",
        "            if time_since_last_call < min_seconds_between_calls:\n",
        "                sleep_time = min_seconds_between_calls - time_since_last_call\n",
        "                time.sleep(sleep_time)\n",
        "    def save_model_state(self, file_path):\n",
        "        \"\"\"Save the model state to a file using numpy's compressed format\"\"\"\n",
        "        save_dict = {}\n",
        "        for model_name, model in self.models.items():\n",
        "            save_dict[f'A_{model_name}'] = model['A']\n",
        "            save_dict[f'b_{model_name}'] = model['b']\n",
        "\n",
        "        # Save to compressed numpy format\n",
        "        np.savez_compressed(file_path, **save_dict)\n",
        "\n",
        "        # Save cost estimator state\n",
        "        cost_estimator_path = file_path.replace('.npz', '_cost_estimator.npz')\n",
        "        self.cost_estimator.save_state(cost_estimator_path)\n",
        "\n",
        "    def load_model_state(self, file_path):\n",
        "        \"\"\"Load the model state from a file\"\"\"\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "\n",
        "            for model_name in self.models.keys():\n",
        "                if f'A_{model_name}' in loaded and f'b_{model_name}' in loaded:\n",
        "                    self.models[model_name]['A'] = loaded[f'A_{model_name}']\n",
        "                    self.models[model_name]['b'] = loaded[f'b_{model_name}']\n",
        "\n",
        "            print(f\"Loaded LinUCB model state from {file_path}\")\n",
        "\n",
        "            # Load cost estimator state\n",
        "            cost_estimator_path = file_path.replace('.npz', '_cost_estimator.npz')\n",
        "            if os.path.exists(cost_estimator_path):\n",
        "                self.cost_estimator.load_state(cost_estimator_path)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "class BatchBudgetCascade:\n",
        "    def __init__(self, feature_extractor, linucb_model, cascade_length=CASCADE_LENGTH):\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.linucb_model = linucb_model\n",
        "        self.cascade_length = cascade_length\n",
        "        self.used_models_for_this_question = []\n",
        "\n",
        "    def grade_with_gemma12b(self, llm_raw_answer, ground_truth_latex):\n",
        "        \"\"\"Grade an answer against the ground truth using the grader model via OpenRouter.\"\"\"\n",
        "        if not llm_raw_answer or not llm_raw_answer.strip():\n",
        "            return False\n",
        "        if llm_raw_answer == ground_truth_latex:\n",
        "            return True\n",
        "\n",
        "        prompt = f\"Expression 1: {llm_raw_answer}\\nExpression 2: {ground_truth_latex}\\n\\n\"\n",
        "        prompt += \"Expression 2 is the answer and expression is attempt by student, look at their final answer only which might be boxed, does student get the final expected answer？ Respond with only the word 'True' or 'False'.\"\n",
        "        try:\n",
        "            time.sleep(0.5) # Simple rate limiting for the grader\n",
        "            api_response = openrouter_client.chat.completions.create(\n",
        "                model=GRADER_MODEL_NAME,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0, max_tokens=10\n",
        "            )\n",
        "            grader_response_text = api_response.choices[0].message.content.strip().lower()\n",
        "            if \"true\" in grader_response_text: return True\n",
        "            elif \"false\" in grader_response_text: return False\n",
        "            else:\n",
        "                print(f\"Warning: Grader returned ambiguous response: {grader_response_text}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling grader model {GRADER_MODEL_NAME} via OpenRouter: {e}\")\n",
        "            return False\n",
        "\n",
        "    def format_prompt(self, question, failed_answers=None, failed_llm_ids=None):\n",
        "        prompt = f\"Solve the following math problem: {question['problem']}\\n\\n\"\n",
        "        prompt += \"Also provide an explnation in one/serveral very short yet consice complete sentence within 75 words in total.\\n\"\n",
        "        prompt += \"At the end, clearly state your final answer in LaTeX format, enclosed within \\\\boxed{}.\\n\"\n",
        "        prompt += \"For example: 'The final answer is \\\\boxed{x=5}'.\"\n",
        "        if failed_answers and failed_llm_ids:\n",
        "            prompt += \"\\nNote: The following previous attempts were incorrect. Please provide a different solution:\\n\"\n",
        "            for i in range(min(len(failed_answers), len(failed_llm_ids))):\n",
        "                prompt += f\"- Attempt {i+1} (by {failed_llm_ids[i]}) led to: {failed_answers[i]}\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def calculate_token_cost(self, model_name, prompt, response_text, usage_info=None):\n",
        "        \"\"\"Calculate the actual cost. For OpenRouter, relies on usage_info from API response.\"\"\"\n",
        "        total_cost = 0.0\n",
        "        error_message = None\n",
        "        if model_name in MODELS_CONFIG:\n",
        "            model_cfg = MODELS_CONFIG[model_name]\n",
        "            if usage_info:\n",
        "                input_tokens = usage_info.get(\"prompt_tokens\", 0)\n",
        "                output_tokens = usage_info.get(\"completion_tokens\", 0)\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "            else:\n",
        "                error_message = f\"Usage info not available for {model_name}. Cost is a rough estimate.\"\n",
        "                input_tokens = len(prompt) // 4\n",
        "                output_tokens = len(response_text) // 4 if response_text else 0\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "        else:\n",
        "            error_message = f\"Model {model_name} not found in config for cost calculation.\"\n",
        "        result = {\"total_cost\": total_cost}\n",
        "        if error_message:\n",
        "            result[\"error\"] = error_message\n",
        "            print(f\"Cost calculation warning for {model_name}: {error_message}\")\n",
        "        return result\n",
        "\n",
        "    def query_llm(self, model_name, prompt):\n",
        "        \"\"\"Query the specified LLM and return its response and cost.\"\"\"\n",
        "        answer_text, cost_data = \"\", {}\n",
        "        try:\n",
        "            self.linucb_model.respect_rate_limit(model_name)\n",
        "            if model_name in MODELS_CONFIG:\n",
        "                api_response = openrouter_client.chat.completions.create(\n",
        "                    model=model_name,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                )\n",
        "                answer_text = api_response.choices[0].message.content.strip()\n",
        "                usage_info = api_response.usage.model_dump() if api_response.usage else None\n",
        "                cost_data = self.calculate_token_cost(model_name, prompt, answer_text, usage_info=usage_info)\n",
        "            else:\n",
        "                raise ValueError(f\"Model {model_name} is not configured in MODELS_CONFIG.\")\n",
        "            self.linucb_model.register_model_call(model_name)\n",
        "            return answer_text, answer_text, cost_data # Returns (raw_response, parsed_answer, cost_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying LLM {model_name}: {e}\")\n",
        "            self.linucb_model.register_model_call(model_name)\n",
        "            cost_data = self.calculate_token_cost(model_name, prompt, \"\", usage_info=None)\n",
        "            return \"\", None, cost_data\n",
        "\n",
        "    def run_cascade_single_question(self, question, question_cost_estimator, is_training=False, initial_question_budget=None):\n",
        "        \"\"\"\n",
        "        Run the cascade with knapsack-based model selection for a single question.\n",
        "        - is_training: If True, use infinite budget and no LLM reselection.\n",
        "        - initial_question_budget: Budget for this question (used in testing).\n",
        "        \"\"\"\n",
        "        base_features = self.feature_extractor.extract_features(question)\n",
        "        failed_answers, failed_llm_ids, self.used_models_for_this_question = [], [], []\n",
        "        question_total_cost, current_attempts_log = 0.0, []\n",
        "        final_status = \"Failure\"\n",
        "\n",
        "        # Set budget based on training/testing mode\n",
        "        if is_training:\n",
        "            current_remaining_budget = float('inf')\n",
        "            print(f\"Training Mode - Unlimited Budget\")\n",
        "        else:\n",
        "            current_remaining_budget = initial_question_budget if initial_question_budget is not None else question_cost_estimator.estimate_question_lcb_cost()\n",
        "            print(f\"Test Mode - Initial Budget: ${current_remaining_budget:.8f}\")\n",
        "\n",
        "        # Knapsack-based Cascade Loop\n",
        "        for i in range(1, self.cascade_length + 1):\n",
        "            print(f\"Step {i}\")\n",
        "            if not is_training and current_remaining_budget <= EPSILON_VD:\n",
        "                print(f\"  Insufficient budget remaining (${current_remaining_budget:.8f}). Aborting cascade.\")\n",
        "                break\n",
        "\n",
        "            x_i = self.feature_extractor.construct_feature_vector(base_features, i, failed_answers, failed_llm_ids, self.linucb_model.model_name_to_index)\n",
        "            available_llms_for_step = [m for m in AVAILABLE_LLMS if m not in self.used_models_for_this_question] if is_training else list(AVAILABLE_LLMS)\n",
        "\n",
        "            # Prepare items for knapsack by estimating value (UCB) and weight (LCB cost)\n",
        "            knapsack_items, llm_info_for_step = [], {}\n",
        "            ucb_scores = self.linucb_model.calculate_ucb_scores(x_i)\n",
        "            knapsack_capacity = float('inf') if is_training else current_remaining_budget\n",
        "\n",
        "            for model_name in available_llms_for_step:\n",
        "                if model_name not in ucb_scores: continue\n",
        "                lcb_cost = self.linucb_model.cost_estimator.estimate_lcb_cost(model_name, i)\n",
        "                if is_training or lcb_cost <= knapsack_capacity:\n",
        "                    knapsack_items.append({\"name\": model_name, \"value\": ucb_scores[model_name][\"ucb_score\"], \"weight\": lcb_cost})\n",
        "                llm_info_for_step[model_name] = {\"ucb_score\": ucb_scores[model_name][\"ucb_score\"], \"lcb_cost\": lcb_cost}\n",
        "\n",
        "            if not knapsack_items:\n",
        "                print(\"  No models fit within the remaining budget for this step.\")\n",
        "                continue\n",
        "\n",
        "            # In training, select best UCB; in testing, use knapsack solver\n",
        "            if is_training:\n",
        "                chosen_llm_names = [max(knapsack_items, key=lambda x: x['value'])['name']]\n",
        "            else:\n",
        "                chosen_llm_names = solve_0_1_knapsack(knapsack_items, knapsack_capacity)\n",
        "\n",
        "            if not chosen_llm_names:\n",
        "                print(\"  Knapsack solution is empty. No combination fits within budget.\")\n",
        "                continue\n",
        "\n",
        "            # Execute chosen subset sequentially, ordered by UCB score\n",
        "            step_successful = False\n",
        "            chosen_llms_sorted = sorted(chosen_llm_names, key=lambda name: llm_info_for_step[name][\"ucb_score\"], reverse=True)\n",
        "            print(f\"  Attempting LLMs in UCB score order: {chosen_llms_sorted}\")\n",
        "\n",
        "            for model_name in chosen_llms_sorted:\n",
        "                lcb_cost = llm_info_for_step[model_name][\"lcb_cost\"]\n",
        "                if not is_training and current_remaining_budget < lcb_cost:\n",
        "                    print(f\"    Skipping {model_name}: Estimated cost (${lcb_cost:.8f}) exceeds remaining budget (${current_remaining_budget:.8f}).\")\n",
        "                    continue\n",
        "\n",
        "                prompt = self.format_prompt(question, failed_answers, failed_llm_ids)\n",
        "                raw_response, parsed_answer, cost_data = self.query_llm(model_name, prompt)\n",
        "                actual_cost = cost_data.get(\"total_cost\", 0.0)\n",
        "                question_total_cost += actual_cost\n",
        "                if not is_training:\n",
        "                    current_remaining_budget -= actual_cost\n",
        "\n",
        "                self.linucb_model.cost_estimator.update_stats(model_name, i, actual_cost)\n",
        "                is_correct = self.grade_with_gemma12b(raw_response, question['answer'])\n",
        "                reward = 1 if is_correct else 0\n",
        "                self.linucb_model.update_reward_only(model_name, x_i, reward)\n",
        "                self.used_models_for_this_question.append(model_name)\n",
        "                print(f\"    {model_name}: Correct={is_correct}, Cost=${actual_cost:.8f}\")\n",
        "\n",
        "                if is_correct:\n",
        "                    step_successful = True\n",
        "                    final_status = \"Success\"\n",
        "                    break # Success, break from inner sequential loop\n",
        "\n",
        "            current_attempts_log.append({\"step\": i, \"chosen_subset\": chosen_llm_names, \"is_successful\": step_successful})\n",
        "            if step_successful:\n",
        "                break # Success, break from outer cascade loop\n",
        "\n",
        "        question_cost_estimator.update_stats(question_total_cost)\n",
        "        return {\n",
        "            \"problem\": question[\"problem\"], \"ground_truth_answer\": question[\"answer\"],\n",
        "            \"final_status\": final_status, \"total_cost\": question_total_cost,\n",
        "            \"initial_question_budget\": initial_question_budget if not is_training else \"Unlimited\",\n",
        "            \"steps_taken\": len(current_attempts_log), \"knapsack_steps\": current_attempts_log,\n",
        "            \"training_mode\": is_training\n",
        "        }"
      ],
      "metadata": {
        "id": "Bha20TRxjgLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_dataset(json_path):\n",
        "    \"\"\"Load the MCQ dataset from a JSON file with robust error handling\"\"\"\n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            raw_data = json.load(f) # Load the raw JSON object\n",
        "\n",
        "        if \"test\" in raw_data and isinstance(raw_data[\"test\"], list):\n",
        "            data = raw_data[\"test\"] # Extract the list of questions from the \"test\" key\n",
        "        else:\n",
        "            print(f\"Warning: 'test' key not found or not a list in {json_path}. Assuming flat list structure or empty.\")\n",
        "            data = raw_data if isinstance(raw_data, list) else [] # Fallback or handle as error\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Dataset file {json_path} not found.\")\n",
        "        return []\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from {json_path}. File may be corrupted.\")\n",
        "        try:\n",
        "            with open(json_path, 'r') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Simple recovery attempt - find all complete JSON objects\n",
        "            import re\n",
        "            pattern = r'\\{[^{}]*\\}'\n",
        "            matches = re.findall(pattern, content)\n",
        "\n",
        "            if matches:\n",
        "                print(f\"Attempted to recover {len(matches)} JSON objects.\")\n",
        "                recovered_data = []\n",
        "                for match in matches:\n",
        "                    try:\n",
        "                        obj = json.loads(match)\n",
        "                        recovered_data.append(obj)\n",
        "                    except:\n",
        "                        pass\n",
        "                return recovered_data\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return []\n",
        "\n",
        "def save_records_with_backup(records, json_path):\n",
        "    \"\"\"Save records to a JSON file with backup of previous file\"\"\"\n",
        "    # Create backup of existing file if it exists\n",
        "    if os.path.exists(json_path):\n",
        "        backup_path = json_path + BACKUP_SUFFIX\n",
        "        try:\n",
        "            os.replace(json_path, backup_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create backup: {e}\")\n",
        "\n",
        "    # Save new data\n",
        "    try:\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(records, f, indent=4)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving records: {e}\")\n",
        "\n",
        "        # Try to restore from backup if save failed\n",
        "        if os.path.exists(json_path + BACKUP_SUFFIX):\n",
        "            try:\n",
        "                os.replace(json_path + BACKUP_SUFFIX, json_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return False\n",
        "\n",
        "def initialize_json_files():\n",
        "    \"\"\"Initialize the JSON files for records with validation\"\"\"\n",
        "    if not os.path.exists(RECORDS_PATH):\n",
        "        with open(RECORDS_PATH, 'w') as f:\n",
        "            json.dump([], f)  # Empty array\n",
        "    else:\n",
        "        # Validate existing file\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            if not isinstance(data, list):\n",
        "                os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
        "                with open(RECORDS_PATH, 'w') as f:\n",
        "                    json.dump([], f)\n",
        "        except json.JSONDecodeError:\n",
        "            os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
        "            with open(RECORDS_PATH, 'w') as f:\n",
        "                json.dump([], f)\n",
        "\n",
        "def analyze_results(records_to_analyze, question_cost_estimator=None):\n",
        "    \"\"\"Analyze and print results from the records\"\"\"\n",
        "    if not records_to_analyze:\n",
        "        print(\"No records to analyze\")\n",
        "        return\n",
        "\n",
        "    # Extract data for analysis\n",
        "    total_questions = len(records_to_analyze)\n",
        "    successful_questions = sum(1 for r in records_to_analyze if r[\"final_status\"] == \"Success\")\n",
        "    success_rate = successful_questions / total_questions if total_questions > 0 else 0\n",
        "\n",
        "    # Calculate success by position (knapsack step)\n",
        "    successes_by_position = [0] * CASCADE_LENGTH\n",
        "    for record in records_to_analyze:\n",
        "        if record[\"final_status\"] == \"Success\":\n",
        "            # Find the successful step (step is 1-indexed)\n",
        "            for i, step in enumerate(record[\"knapsack_steps\"]):\n",
        "                if step[\"is_successful\"]:\n",
        "                    # Convert to 0-indexed for array\n",
        "                    position = i\n",
        "                    if position < CASCADE_LENGTH:\n",
        "                        successes_by_position[position] += 1\n",
        "                    break\n",
        "\n",
        "    per_position_success = [count/total_questions for count in successes_by_position]\n",
        "\n",
        "    # Calculate average steps and costs\n",
        "    total_steps = sum(r[\"steps_taken\"] for r in records_to_analyze)\n",
        "    total_cost = sum(r[\"total_cost\"] for r in records_to_analyze)\n",
        "\n",
        "    avg_steps = total_steps / total_questions if total_questions > 0 else 0\n",
        "    avg_cost = total_cost / total_questions if total_questions > 0 else 0\n",
        "\n",
        "    # Calculate average cost for successful questions only\n",
        "    if successful_questions > 0:\n",
        "        success_cost = sum(r[\"total_cost\"] for r in records_to_analyze if r[\"final_status\"] == \"Success\")\n",
        "        avg_cost_success = success_cost / successful_questions\n",
        "    else:\n",
        "        avg_cost_success = 0\n",
        "\n",
        "    # Analyze model performance - modified for knapsack approach\n",
        "    model_metrics = {model: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0, \"by_position\": defaultdict(lambda: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0})}\n",
        "                    for model in AVAILABLE_LLMS}\n",
        "\n",
        "    # Track LCB cost estimation accuracy\n",
        "    lcb_cost_accuracy = {model: {\"total_lcb_estimate\": 0.0, \"total_actual_cost\": 0.0, \"count\": 0}\n",
        "                         for model in AVAILABLE_LLMS}\n",
        "\n",
        "    # Track knapsack metrics\n",
        "    knapsack_metrics = {\n",
        "        \"total_steps\": 0,\n",
        "        \"avg_models_per_step\": 0,\n",
        "        \"models_selected_count\": 0,\n",
        "        \"budget_utilization\": 0.0,\n",
        "        \"total_budget_allocated\": 0.0,\n",
        "        \"by_position\": defaultdict(lambda: {\"steps\": 0, \"models_selected\": 0, \"budget_allocated\": 0.0, \"actual_cost\": 0.0})\n",
        "    }\n",
        "\n",
        "    for record in records_to_analyze:\n",
        "        for step in record[\"knapsack_steps\"]:\n",
        "            position = step[\"step\"]  # 1-indexed\n",
        "            knapsack_metrics[\"total_steps\"] += 1\n",
        "            knapsack_metrics[\"models_selected_count\"] += len(step[\"chosen_subset_by_knapsack\"])\n",
        "\n",
        "            # Handle the case where step_budget can be \"Unlimited\" string\n",
        "            if step[\"knapsack_capacity_for_step\"] != \"Unlimited\":\n",
        "                step_budget = float(step[\"knapsack_capacity_for_step\"])\n",
        "                knapsack_metrics[\"total_budget_allocated\"] += step_budget\n",
        "\n",
        "                # Update position-specific budget stats\n",
        "                pos_stats = knapsack_metrics[\"by_position\"][position]\n",
        "                pos_stats[\"budget_allocated\"] += step_budget\n",
        "            else:\n",
        "                # Skip budget calculations for unlimited budget steps\n",
        "                step_budget = float('inf')\n",
        "\n",
        "            # Update position-specific knapsack stats\n",
        "            pos_stats = knapsack_metrics[\"by_position\"][position]\n",
        "            pos_stats[\"steps\"] += 1\n",
        "            pos_stats[\"models_selected\"] += len(step[\"chosen_subset_by_knapsack\"])\n",
        "            pos_stats[\"actual_cost\"] += step[\"actual_cost_for_step\"]\n",
        "\n",
        "            # Process individual LLM attempts\n",
        "            for attempt in step[\"llm_attempts\"]:\n",
        "                model = attempt[\"knapsack_model\"]\n",
        "                is_correct = attempt[\"is_correct\"]\n",
        "                cost = attempt[\"actual_cost\"]\n",
        "\n",
        "                # Track LCB cost estimation accuracy\n",
        "                if \"lcb_cost_estimate\" in attempt:\n",
        "                    lcb_cost_accuracy[model][\"total_lcb_estimate\"] += attempt[\"lcb_cost_estimate\"]\n",
        "                    lcb_cost_accuracy[model][\"total_actual_cost\"] += cost\n",
        "                    lcb_cost_accuracy[model][\"count\"] += 1\n",
        "\n",
        "                # Update overall model stats\n",
        "                model_metrics[model][\"calls\"] += 1\n",
        "                model_metrics[model][\"total_cost\"] += cost\n",
        "                if is_correct:\n",
        "                    model_metrics[model][\"successes\"] += 1\n",
        "\n",
        "                # Update position-specific stats\n",
        "                model_metrics[model][\"by_position\"][position][\"calls\"] += 1\n",
        "                model_metrics[model][\"by_position\"][position][\"total_cost\"] += cost\n",
        "                if is_correct:\n",
        "                    model_metrics[model][\"by_position\"][position][\"successes\"] += 1\n",
        "\n",
        "    # Calculate average models per step\n",
        "    if knapsack_metrics[\"total_steps\"] > 0:\n",
        "        knapsack_metrics[\"avg_models_per_step\"] = knapsack_metrics[\"models_selected_count\"] / knapsack_metrics[\"total_steps\"]\n",
        "        if knapsack_metrics[\"total_budget_allocated\"] > 0:\n",
        "            knapsack_metrics[\"budget_utilization\"] = sum(r[\"total_cost\"] for r in records_to_analyze) / knapsack_metrics[\"total_budget_allocated\"]\n",
        "\n",
        "    # Calculate success rates and average costs for models\n",
        "    for model, data in model_metrics.items():\n",
        "        if data[\"calls\"] > 0:\n",
        "            data[\"success_rate\"] = data[\"successes\"] / data[\"calls\"]\n",
        "            data[\"avg_cost\"] = data[\"total_cost\"] / data[\"calls\"]\n",
        "        else:\n",
        "            data[\"success_rate\"] = 0\n",
        "            data[\"avg_cost\"] = 0\n",
        "\n",
        "        for position, pos_data in data[\"by_position\"].items():\n",
        "            if pos_data[\"calls\"] > 0:\n",
        "                pos_data[\"success_rate\"] = pos_data[\"successes\"] / pos_data[\"calls\"]\n",
        "                pos_data[\"avg_cost\"] = pos_data[\"total_cost\"] / pos_data[\"calls\"]\n",
        "            else:\n",
        "                pos_data[\"success_rate\"] = 0\n",
        "                pos_data[\"avg_cost\"] = 0\n",
        "\n",
        "    # Calculate LCB estimation accuracy\n",
        "    for model, data in lcb_cost_accuracy.items():\n",
        "        if data[\"count\"] > 0:\n",
        "            data[\"avg_lcb_estimate\"] = data[\"total_lcb_estimate\"] / data[\"count\"]\n",
        "            data[\"avg_actual_cost\"] = data[\"total_actual_cost\"] / data[\"count\"]\n",
        "            data[\"lcb_accuracy\"] = data[\"avg_lcb_estimate\"] / data[\"avg_actual_cost\"] if data[\"avg_actual_cost\"] > 0 else 0\n",
        "        else:\n",
        "            data[\"avg_lcb_estimate\"] = 0\n",
        "            data[\"avg_actual_cost\"] = 0\n",
        "            data[\"lcb_accuracy\"] = 0\n",
        "\n",
        "    # Calculate position-specific knapsack metrics\n",
        "    for position, data in knapsack_metrics[\"by_position\"].items():\n",
        "        if data[\"steps\"] > 0:\n",
        "            data[\"avg_models_per_step\"] = data[\"models_selected\"] / data[\"steps\"]\n",
        "\n",
        "            if data[\"budget_allocated\"] > 0:\n",
        "                data[\"avg_budget\"] = data[\"budget_allocated\"] / data[\"steps\"]\n",
        "                data[\"budget_utilization\"] = data[\"actual_cost\"] / data[\"budget_allocated\"]\n",
        "            else:\n",
        "                data[\"avg_budget\"] = \"Unlimited\"\n",
        "                data[\"budget_utilization\"] = 0\n",
        "\n",
        "            data[\"avg_cost\"] = data[\"actual_cost\"] / data[\"steps\"]\n",
        "\n",
        "    # Analyze batch performance\n",
        "    batch_metrics = []\n",
        "    for i in range(0, len(records_to_analyze), BATCH_SIZE):\n",
        "        batch = records_to_analyze[i:i+BATCH_SIZE]\n",
        "        batch_success = sum(1 for r in batch if r[\"final_status\"] == \"Success\")\n",
        "        batch_success_rate = batch_success / len(batch) if batch else 0\n",
        "        batch_cost = sum(r[\"total_cost\"] for r in batch)\n",
        "        batch_avg_cost = batch_cost / len(batch) if batch else 0\n",
        "\n",
        "        batch_metrics.append({\n",
        "            \"batch_idx\": i // BATCH_SIZE,\n",
        "            \"batch_size\": len(batch),\n",
        "            \"success_count\": batch_success,\n",
        "            \"success_rate\": batch_success_rate,\n",
        "            \"total_cost\": batch_cost,\n",
        "            \"avg_cost\": batch_avg_cost\n",
        "        })\n",
        "\n",
        "    # Generate summary text\n",
        "    dataset_type = \"TRAIN\" if records_to_analyze and records_to_analyze[0].get(\"dataset\") == \"train\" else \"TEST\" if records_to_analyze and records_to_analyze[0].get(\"dataset\") == \"test\" else \"OVERALL\"\n",
        "    summary = f\"=== LinUCB CASCADE WITH KNAPSACK SELECTION - {dataset_type} SET RESULTS ===\\n\\n\"\n",
        "    summary += f\"Batch Size: {BATCH_SIZE}\\n\"\n",
        "    summary += f\"LLM Cost Beta: {BETA_COST}\\n\"\n",
        "    summary += f\"Question Cost Beta: {BETA_COST_QUESTION}\\n\"\n",
        "    summary += f\"Cascade Length (K): {CASCADE_LENGTH}\\n\\n\"\n",
        "\n",
        "    summary += \"=== OVERALL RESULTS ===\\n\"\n",
        "    summary += f\"Total Questions: {total_questions}\\n\"\n",
        "    summary += f\"Success Rate: {success_rate:.4f}\\n\"\n",
        "    summary += f\"Average Steps: {avg_steps:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${avg_cost:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${avg_cost_success:.8f}\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(per_position_success):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    # Add question cost estimation info if available\n",
        "    if question_cost_estimator and question_cost_estimator.stats[\"count\"] > 0:\n",
        "        avg_question_cost = question_cost_estimator.stats[\"total_cost\"] / question_cost_estimator.stats[\"count\"]\n",
        "        estimated_lcb = question_cost_estimator.estimate_question_lcb_cost()\n",
        "\n",
        "        summary += \"\\n=== QUESTION COST ESTIMATION ===\\n\"\n",
        "        summary += f\"Total Questions Processed: {question_cost_estimator.stats['count']}\\n\"\n",
        "        summary += f\"Average Question Cost: ${avg_question_cost:.8f}\\n\"\n",
        "        summary += f\"Current LCB Question Cost Estimate: ${estimated_lcb:.8f}\\n\"\n",
        "\n",
        "    # Add knapsack metrics\n",
        "    summary += \"\\n=== KNAPSACK METRICS ===\\n\"\n",
        "    summary += f\"Average Models Selected per Step: {knapsack_metrics['avg_models_per_step']:.4f}\\n\"\n",
        "    if knapsack_metrics[\"total_budget_allocated\"] > 0:\n",
        "        summary += f\"Overall Budget Utilization: {knapsack_metrics['budget_utilization']:.4f}\\n\"\n",
        "    summary += \"By Position:\\n\"\n",
        "    for position, data in sorted(knapsack_metrics[\"by_position\"].items()):\n",
        "        if data[\"steps\"] > 0:\n",
        "            summary += f\"  Position {position}: {data['avg_models_per_step']:.2f} models/step, \"\n",
        "            if data[\"avg_budget\"] != \"Unlimited\":\n",
        "                summary += f\"Budget=${data['avg_budget']:.8f}, \"\n",
        "                summary += f\"Utilization={data['budget_utilization']:.4f}, \"\n",
        "            else:\n",
        "                summary += f\"Budget=Unlimited, \"\n",
        "            summary += f\"Cost=${data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== MODEL PERFORMANCE ===\\n\"\n",
        "    for model, metrics in model_metrics.items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"\\n{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "            summary += \"  By Position:\\n\"\n",
        "            for position, pos_data in sorted(metrics[\"by_position\"].items()):\n",
        "                if pos_data[\"calls\"] > 0:\n",
        "                    summary += f\"    Pos {position}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== LLM LCB COST ESTIMATION ACCURACY ===\\n\"\n",
        "    for model, data in lcb_cost_accuracy.items():\n",
        "        if data[\"count\"] > 0:\n",
        "            summary += f\"{model}: Avg LCB Est: ${data['avg_lcb_estimate']:.8f}, Avg Actual: ${data['avg_actual_cost']:.8f}, Ratio: {data['lcb_accuracy']:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== BATCH PERFORMANCE ===\\n\"\n",
        "    for batch in batch_metrics:\n",
        "        summary += f\"Batch {batch['batch_idx']}: {batch['success_count']}/{batch['batch_size']} = {batch['success_rate']:.4f}, Total Cost: ${batch['total_cost']:.8f}, Avg Cost: ${batch['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += f\"\\nTotal Overall Cost (All Questions): ${total_cost:.8f}\\n\"\n",
        "\n",
        "    # Print and save summary\n",
        "    print(summary)\n",
        "\n",
        "    # Save to appropriate file based on dataset type\n",
        "    summary_file_path = SUMMARY_STATS_PATH.replace(\".txt\", f\"_{dataset_type}.txt\")\n",
        "    with open(summary_file_path, 'w') as f:\n",
        "        f.write(summary)\n",
        "\n",
        "\n",
        "def select_questions_by_lcb_cost(remaining_questions, question_cost_estimator, batch_size, pool_multiplier=QUESTION_POOL_MULTIPLIER):\n",
        "    \"\"\"\n",
        "    Select the next batch of questions based on LCB cost estimates\n",
        "\n",
        "    Parameters:\n",
        "    - remaining_questions: List of unprocessed questions\n",
        "    - question_cost_estimator: The estimator for question costs\n",
        "    - batch_size: Number of questions to select\n",
        "    - pool_multiplier: How many times batch_size to consider\n",
        "\n",
        "    Returns:\n",
        "    - Selected questions for the next batch\n",
        "    \"\"\"\n",
        "    # Determine the pool size - consider at least batch_size questions\n",
        "    pool_size = min(len(remaining_questions), batch_size * pool_multiplier)\n",
        "    if pool_size <= batch_size:\n",
        "        # If we don't have enough questions for a meaningful pool, return the first batch_size\n",
        "        return remaining_questions[:batch_size]\n",
        "\n",
        "    # Get the candidate pool\n",
        "    candidate_pool = remaining_questions[:pool_size]\n",
        "\n",
        "    # Calculate LCB cost estimate for each question\n",
        "    question_costs = []\n",
        "    for i, question in enumerate(candidate_pool):\n",
        "        lcb_cost = question_cost_estimator.estimate_question_lcb_cost()\n",
        "        question_costs.append((i, lcb_cost))\n",
        "\n",
        "    # Sort by LCB cost (ascending)\n",
        "    question_costs.sort(key=lambda x: x[1])\n",
        "\n",
        "    # Get the indices of the lowest-cost questions\n",
        "    selected_indices = [idx for idx, _ in question_costs[:batch_size]]\n",
        "    selected_indices.sort()  # Sort indices to preserve original order\n",
        "\n",
        "    # Select the questions\n",
        "    selected_questions = [candidate_pool[idx] for idx in selected_indices]\n",
        "\n",
        "    # Get the remaining questions (those not in the pool + those in pool but not selected)\n",
        "    non_selected_pool_indices = set(range(pool_size)) - set(selected_indices)\n",
        "    non_selected_pool = [candidate_pool[idx] for idx in non_selected_pool_indices]\n",
        "    remaining_after_pool = remaining_questions[pool_size:]\n",
        "\n",
        "    # Update the remaining_questions list (in-place)\n",
        "    remaining_questions.clear()\n",
        "    remaining_questions.extend(selected_questions)  # Put selected questions first\n",
        "    remaining_questions.extend(non_selected_pool)   # Then non-selected from pool\n",
        "    remaining_questions.extend(remaining_after_pool)  # Then the rest\n",
        "\n",
        "    return selected_questions\n"
      ],
      "metadata": {
        "id": "gOKttQvHkXec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVVz9ADfilkN"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"Starting LinUCB Cascade with Knapsack Selection\")\n",
        "    initialize_json_files()\n",
        "    dataset = load_dataset(INPUT_JSON)\n",
        "    if not dataset:\n",
        "        print(\"Dataset is empty or could not be loaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    feature_extractor = FeatureExtractor()\n",
        "    feature_extractor.initialize(dataset)\n",
        "    linucb_model = LinUCBModel(model_names=AVAILABLE_LLMS)\n",
        "    question_cost_estimator = QuestionCostEstimator()\n",
        "    if os.path.exists(QUESTION_COST_PATH):\n",
        "        question_cost_estimator.load_state(QUESTION_COST_PATH)\n",
        "    cascade = BatchBudgetCascade(feature_extractor, linucb_model)\n",
        "\n",
        "    all_records = []\n",
        "    if os.path.exists(RECORDS_PATH):\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f: all_records = json.load(f)\n",
        "            processed_questions = {r[\"problem\"] for r in all_records}\n",
        "            dataset = [q for q in dataset if q[\"problem\"] not in processed_questions]\n",
        "            print(f\"Loaded {len(all_records)} existing records. {len(dataset)} new questions to process.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load existing records: {e}\")\n",
        "\n",
        "    if os.path.exists(LINUCB_MODEL_PATH):\n",
        "        linucb_model.load_model_state(LINUCB_MODEL_PATH)\n",
        "\n",
        "    train_size = int(len(dataset) * 0.2)\n",
        "    train_dataset, test_dataset = dataset[:train_size], dataset[train_size:]\n",
        "    print(f\"Split dataset into {len(train_dataset)} training and {len(test_dataset)} testing questions.\")\n",
        "\n",
        "    try:\n",
        "        # --- Training Phase ---\n",
        "        print(\"\\n=== PROCESSING TRAINING SET ===\")\n",
        "        for idx, question in enumerate(train_dataset):\n",
        "            print(f\"\\nProcessing Training Question #{idx + 1}/{len(train_dataset)}\")\n",
        "            question_record = cascade.run_cascade_single_question(question, question_cost_estimator, is_training=True)\n",
        "            question_record[\"dataset\"] = \"train\"\n",
        "            all_records.append(question_record)\n",
        "            if (idx + 1) % UPDATE_FREQUENCY == 0:\n",
        "                save_records_with_backup(all_records, RECORDS_PATH)\n",
        "                linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "                question_cost_estimator.save_state(QUESTION_COST_PATH)\n",
        "\n",
        "        # --- Testing Phase ---\n",
        "        print(\"\\n=== PROCESSING TEST SET ===\")\n",
        "        for idx, question in enumerate(test_dataset):\n",
        "            print(f\"\\nProcessing Test Question #{idx + 1}/{len(test_dataset)}\")\n",
        "            # Using a fixed, tuned budget for reproducibility in the test phase\n",
        "            test_budget = 0.00014217\n",
        "            question_record = cascade.run_cascade_single_question(question, question_cost_estimator, is_training=False, initial_question_budget=test_budget)\n",
        "            question_record[\"dataset\"] = \"test\"\n",
        "            all_records.append(question_record)\n",
        "            if (idx + 1) % UPDATE_FREQUENCY == 0:\n",
        "                save_records_with_backup(all_records, RECORDS_PATH)\n",
        "                linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "                question_cost_estimator.save_state(QUESTION_COST_PATH)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProcessing interrupted. Saving progress...\")\n",
        "    finally:\n",
        "        save_records_with_backup(all_records, RECORDS_PATH)\n",
        "        linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "        question_cost_estimator.save_state(QUESTION_COST_PATH)\n",
        "\n",
        "        # Analyze and report results\n",
        "        train_records = [r for r in all_records if r.get(\"dataset\") == \"train\"]\n",
        "        test_records = [r for r in all_records if r.get(\"dataset\") == \"test\"]\n",
        "        print(\"\\n--- FINAL ANALYSIS ---\")\n",
        "        analyze_results(train_records, question_cost_estimator)\n",
        "        analyze_results(test_records, question_cost_estimator)\n",
        "        print(\"\\nLinUCB Cascade with Knapsack Selection completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}