{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import tiktoken\n",
        "import google.generativeai as genai\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import defaultdict\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_FEATURE_DIM = 384  # Dimension for question embeddings\n",
        "ANSWER_EMBED_DIM = 384  # Dimension for answer embeddings\n",
        "CASCADE_LENGTH = 5      # Number of attempts in the cascade (K)\n",
        "UPDATE_FREQUENCY = 1    # Update JSON records after every question\n",
        "USE_EMBEDDINGS = True   # Use embeddings instead of TF-IDF\n",
        "ALPHA = 0.675           # Exploration parameter for LinUCB\n",
        "LAMBDA_REG = 0.45       # Regularization parameter for LinUCB matrix initialization\n",
        "TRAIN_RATIO = 0.2\n",
        "BATCH_SIZE = 20"
      ],
      "metadata": {
        "id": "X0vUp7zpg3qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENROUTER_API_KEY =  # Replace with your Openrouterkey\n",
        "OPENROUTER_BASE_URL =  # Replace with your openrouter URL"
      ],
      "metadata": {
        "id": "h33tyfzEg7e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zzs9BMtfLgr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    \"microsoft/phi-3.5-mini-128k-instruct\": {\"input_cost\": 0.03 / 1e6, \"output_cost\": 0.09 / 1e6},\n",
        "    \"mistralai/mistral-small-3.1-24b-instruct\": {\"input_cost\": 0.05 / 1e6, \"output_cost\": 0.15 / 1e6},\n",
        "    \"microsoft/phi-4\": {\"input_cost\": 0.07 / 1e6, \"output_cost\": 0.14 / 1e6},\n",
        "    \"meta-llama/llama-4-maverick\": {\"input_cost\": 0.17 / 1e6, \"output_cost\": 0.16 / 1e6},\n",
        "    \"google/gemini-2.0-flash-001\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"openai/gpt-4.1-nano\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"deepseek/deepseek-chat\": {\"input_cost\": 0.38 / 1e6, \"output_cost\": 0.89 / 1e6},\n",
        "}\n",
        "\n",
        "GRADER_MODEL_NAME = \"google/gemini-2.0-flash-lite-001\"\n",
        "\n",
        "AVAILABLE_LLMS = list(MODELS_CONFIG.keys())\n",
        "LLM_ID_DIM = len(AVAILABLE_LLMS)\n",
        "\n",
        "# Feature dimensions are recalculated based on the number of available LLMs\n",
        "CONTEXT_FEATURE_DIM = ANSWER_EMBED_DIM + LLM_ID_DIM + ANSWER_EMBED_DIM\n",
        "TOTAL_FEATURE_DIM = BASE_FEATURE_DIM + 1 + CONTEXT_FEATURE_DIM\n",
        "\n",
        "# Initialize OpenRouter client\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=OPENROUTER_BASE_URL,\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        "    # You might need to add default headers if required by your OpenRouter setup/account\n",
        "    # default_headers={\"HTTP-Referer\": \"YOUR_SITE_URL\", \"X-Title\": \"YOUR_APP_NAME\"}\n",
        ")\n",
        "\n",
        "# File paths\n",
        "INPUT_JSON = \"Math500.json\"\n",
        "RECORDS_PATH = \"M91.json\"\n",
        "LINUCB_MODEL_PATH = \"M91.npz\"\n",
        "SUMMARY_STATS_PATH = \"M91.txt\"\n",
        "BACKUP_SUFFIX = \".bak\"\n",
        "\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, feature_dim=BASE_FEATURE_DIM, use_embeddings=USE_EMBEDDINGS):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.use_embeddings = use_embeddings\n",
        "        if use_embeddings:\n",
        "            try:\n",
        "                self.embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
        "                print(\"Initialized sentence transformer embedding model.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error initializing sentence transformer: {e}\\nFalling back to TF-IDF.\")\n",
        "                self.use_embeddings = False\n",
        "        if not self.use_embeddings:\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.svd = None\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, questions):\n",
        "        \"\"\"Initialize the vectorizer with the corpus of questions.\"\"\"\n",
        "        if not self.use_embeddings:\n",
        "            from sklearn.decomposition import TruncatedSVD\n",
        "            all_text = [q[\"question\"] for q in questions]\n",
        "            dtm = self.vectorizer.fit_transform(all_text)\n",
        "            n_components = min(self.feature_dim, dtm.shape[1])\n",
        "            self.svd = TruncatedSVD(n_components=n_components)\n",
        "            self.svd.fit(dtm)\n",
        "            print(f\"Using TF-IDF with SVD dimensionality reduction to {n_components} features.\")\n",
        "        self.initialized = True\n",
        "\n",
        "    def extract_features(self, question):\n",
        "        \"\"\"Extract features from a question.\"\"\"\n",
        "        if not self.initialized:\n",
        "            raise ValueError(\"Feature extractor not initialized. Call initialize() first.\")\n",
        "        text = question[\"question\"]\n",
        "        if self.use_embeddings:\n",
        "            return self.embedding_model.encode([text])[0]\n",
        "        else:\n",
        "            tfidf_vector = self.vectorizer.transform([text])\n",
        "            features = self.svd.transform(tfidf_vector)[0]\n",
        "            if len(features) < self.feature_dim:\n",
        "                padding = np.zeros(self.feature_dim - len(features))\n",
        "                features = np.concatenate([features, padding])\n",
        "            return features\n",
        "\n",
        "    def extract_answer_features(self, answer_text):\n",
        "        \"\"\"Extract features from an answer string.\"\"\"\n",
        "        if not answer_text:\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "        if self.use_embeddings:\n",
        "            try:\n",
        "                features = self.embedding_model.encode([answer_text])[0]\n",
        "                # Handle cases where embedding dimension might not match\n",
        "                if len(features) != ANSWER_EMBED_DIM:\n",
        "                    if len(features) > ANSWER_EMBED_DIM:\n",
        "                        features = features[:ANSWER_EMBED_DIM]\n",
        "                    else:\n",
        "                        padding = np.zeros(ANSWER_EMBED_DIM - len(features))\n",
        "                        features = np.concatenate([features, padding])\n",
        "                return features\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding answer: {e}\")\n",
        "                return np.zeros(ANSWER_EMBED_DIM)\n",
        "        else:\n",
        "            # For simplicity, use zero vector if not using embeddings for answers\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "    def construct_feature_vector(self, base_features, step_i, failed_answers, failed_llm_ids, model_name_to_index):\n",
        "        \"\"\"\n",
        "        Construct the augmented feature vector for LinUCB.\n",
        "        Features include: base question features, normalized step, last answer embedding,\n",
        "        one-hot encoding of the last failed LLM, and an average of all previous failed answer embeddings.\n",
        "        \"\"\"\n",
        "        normalized_step = np.array([step_i / CASCADE_LENGTH])\n",
        "\n",
        "        last_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        if step_i > 1 and failed_answers:\n",
        "            last_answer_features = self.extract_answer_features(failed_answers[-1])\n",
        "\n",
        "        last_llm_onehot = np.zeros(LLM_ID_DIM)\n",
        "        if step_i > 1 and failed_llm_ids:\n",
        "            last_llm_name = failed_llm_ids[-1]\n",
        "            if last_llm_name in model_name_to_index:\n",
        "                last_llm_onehot[model_name_to_index[last_llm_name]] = 1.0\n",
        "\n",
        "        avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        if step_i > 1 and failed_answers:\n",
        "            all_answer_features = [self.extract_answer_features(ans) for ans in failed_answers]\n",
        "            if all_answer_features:\n",
        "                avg_answer_features = np.mean(all_answer_features, axis=0)\n",
        "        if avg_answer_features.shape == (): # Handle scalar result from mean\n",
        "            avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "        context_features = np.concatenate([last_answer_features, last_llm_onehot, avg_answer_features])\n",
        "        augmented_features = np.concatenate([base_features, normalized_step, context_features])\n",
        "\n",
        "        if augmented_features.shape[0] != TOTAL_FEATURE_DIM:\n",
        "            raise ValueError(f\"Constructed feature vector dimension {augmented_features.shape[0]} != expected {TOTAL_FEATURE_DIM}\")\n",
        "        return augmented_features\n",
        "\n",
        "\n",
        "class LinUCBModel:\n",
        "    def __init__(self, model_names, feature_dim=TOTAL_FEATURE_DIM, alpha=ALPHA, lambda_reg=LAMBDA_REG):\n",
        "        self.model_names = model_names\n",
        "        self.feature_dim = feature_dim\n",
        "        self.alpha = alpha\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.model_name_to_index = {name: i for i, name in enumerate(model_names)}\n",
        "        self.models = {\n",
        "            model_name: {\n",
        "                'A': np.identity(feature_dim) * lambda_reg,\n",
        "                'b': np.zeros(feature_dim),\n",
        "                'last_call_time': 0\n",
        "            } for model_name in model_names\n",
        "        }\n",
        "\n",
        "    def update_reward_only(self, model_name, feature_vector, reward):\n",
        "        \"\"\"Update the LinUCB model parameters based on observed reward.\"\"\"\n",
        "        model = self.models[model_name]\n",
        "        model['A'] += np.outer(feature_vector, feature_vector)\n",
        "        model['b'] += feature_vector * reward\n",
        "\n",
        "    def calculate_ucb_scores(self, feature_vector):\n",
        "        \"\"\"Calculate UCB scores for model selection.\"\"\"\n",
        "        scores = {}\n",
        "        for model_name in self.model_names:\n",
        "            model = self.models[model_name]\n",
        "            try:\n",
        "                # Use Cholesky decomposition for faster and more stable inversion\n",
        "                L = np.linalg.cholesky(model['A'])\n",
        "                theta = np.linalg.solve(model['A'], model['b'])\n",
        "                z = np.linalg.solve(L, feature_vector)\n",
        "                ucb_term = self.alpha * np.sqrt(np.sum(z**2))\n",
        "                expected_reward = feature_vector.dot(theta)\n",
        "                scores[model_name] = {\n",
        "                    \"p_ia\": float(expected_reward),\n",
        "                    \"e_ia\": float(ucb_term),\n",
        "                    \"ucb_score\": float(expected_reward + ucb_term)\n",
        "                }\n",
        "            except np.linalg.LinAlgError:\n",
        "                # Fallback to standard matrix inversion if Cholesky fails\n",
        "                try:\n",
        "                    A_inv = np.linalg.inv(model['A'])\n",
        "                    theta = A_inv.dot(model['b'])\n",
        "                    ucb_term = self.alpha * np.sqrt(feature_vector.dot(A_inv).dot(feature_vector))\n",
        "                    expected_reward = feature_vector.dot(theta)\n",
        "                    scores[model_name] = {\n",
        "                        \"p_ia\": float(expected_reward),\n",
        "                        \"e_ia\": float(ucb_term),\n",
        "                        \"ucb_score\": float(expected_reward + ucb_term)\n",
        "                    }\n",
        "                except:\n",
        "                    scores[model_name] = {\"p_ia\": 0.0, \"e_ia\": 0.0, \"ucb_score\": 0.0}\n",
        "        return scores\n",
        "\n",
        "    def select_model_ucb(self, feature_vector):\n",
        "        \"\"\"Select a model using standard LinUCB algorithm.\"\"\"\n",
        "        scores = self.calculate_ucb_scores(feature_vector)\n",
        "        if not scores:\n",
        "            return None, {}\n",
        "        chosen_model = max(scores.items(), key=lambda x: x[1][\"ucb_score\"])[0]\n",
        "        return chosen_model, scores\n",
        "\n",
        "    def register_model_call(self, model_name):\n",
        "        self.models[model_name]['last_call_time'] = time.time()\n",
        "\n",
        "    def respect_rate_limit(self, model_name):\n",
        "        \"\"\"Wait if necessary to respect a model's RPM if defined in config.\"\"\"\n",
        "        # For the specified OpenRouter models, 'rpm' is not defined, so this block will be skipped.\n",
        "        model_cfg = MODELS_CONFIG.get(model_name)\n",
        "        if model_cfg and \"rpm\" in model_cfg:\n",
        "            model_state = self.models[model_name]\n",
        "            min_seconds_between_calls = 60.0 / model_cfg[\"rpm\"]\n",
        "            time_since_last_call = time.time() - model_state['last_call_time']\n",
        "            if time_since_last_call < min_seconds_between_calls:\n",
        "                time.sleep(min_seconds_between_calls - time_since_last_call)\n",
        "\n",
        "    def save_model_state(self, file_path):\n",
        "        \"\"\"Save the model state to a compressed numpy file.\"\"\"\n",
        "        save_dict = {f'A_{model_name}': model['A'] for model_name, model in self.models.items()}\n",
        "        for model_name, model in self.models.items():\n",
        "            save_dict[f'b_{model_name}'] = model['b']\n",
        "        np.savez_compressed(file_path, **save_dict)\n",
        "\n",
        "    def load_model_state(self, file_path):\n",
        "        \"\"\"Load the model state from a file.\"\"\"\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            for model_name in self.models.keys():\n",
        "                if f'A_{model_name}' in loaded and f'b_{model_name}' in loaded:\n",
        "                    self.models[model_name]['A'] = loaded[f'A_{model_name}']\n",
        "                    self.models[model_name]['b'] = loaded[f'b_{model_name}']\n",
        "            print(f\"Loaded LinUCB model state from {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class BatchBudgetCascade:\n",
        "    def __init__(self, feature_extractor, linucb_model, cascade_length=CASCADE_LENGTH):\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.linucb_model = linucb_model\n",
        "        self.cascade_length = cascade_length\n",
        "\n",
        "    def format_prompt(self, question, failed_answers=None, failed_llm_ids=None):\n",
        "        \"\"\"Generate a prompt for the LLM with the math problem and previous failed answers.\"\"\"\n",
        "        prompt = f\"Solve the following math problem: {question['question']}\\n\\n\"\n",
        "        prompt += \"Please provide a Structured derivation of your answer within 50 words.\\n\"\n",
        "        prompt += \"At the end, clearly state your final answer in LaTeX format, enclosed within \\\\boxed{}.\\n\"\n",
        "        prompt += \"For example: 'The final answer is \\\\boxed{x=5}'.\"\n",
        "        if failed_answers and failed_llm_ids:\n",
        "            prompt += \"\\n\\nNote: The following previous attempts were incorrect. Please provide a different solution:\\n\"\n",
        "            for i, answer_info in enumerate(failed_answers):\n",
        "                prompt += f\"- Attempt {i+1} (by {failed_llm_ids[i]}) led to: {answer_info}\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def parse_llm_answer(self, answer_text):\n",
        "        \"\"\"\n",
        "        Returns the raw, stripped answer text for grading. This setup is to ensure\n",
        "        the grader evaluates the model's full output, not just a parsed LaTeX block.\n",
        "        \"\"\"\n",
        "        if answer_text is None:\n",
        "            return None, \"\"\n",
        "        raw_stripped_answer = answer_text.strip()\n",
        "        return raw_stripped_answer, raw_stripped_answer\n",
        "\n",
        "    def grade_with_gemma12b(self, llm_answer_latex, ground_truth_latex):\n",
        "        \"\"\"Grade an answer against the ground truth using the grader model via OpenRouter.\"\"\"\n",
        "        if llm_answer_latex is None or llm_answer_latex == ground_truth_latex:\n",
        "            return llm_answer_latex is not None\n",
        "\n",
        "        prompt = f\"Expression 1: {llm_answer_latex}\\nExpression 2: {ground_truth_latex}\\n\\n\"\n",
        "        prompt += \"Expression 2 is the answer and expression is attempt by student, look at their final answer only which might be boxed, does student get the final expected answer？ Respond with only the word 'True' or 'False'.\"\n",
        "        try:\n",
        "            time.sleep(0.5) # Simple rate limiting for the grader\n",
        "            api_response = openrouter_client.chat.completions.create(\n",
        "                model=GRADER_MODEL_NAME,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0,\n",
        "                max_tokens=10\n",
        "            )\n",
        "            grader_response_text = api_response.choices[0].message.content.strip().lower()\n",
        "            if \"true\" in grader_response_text:\n",
        "                return True\n",
        "            elif \"false\" in grader_response_text:\n",
        "                return False\n",
        "            else:\n",
        "                print(f\"Warning: Grader ({GRADER_MODEL_NAME}) returned ambiguous response: {grader_response_text}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling grader model {GRADER_MODEL_NAME} via OpenRouter: {e}\")\n",
        "            return False\n",
        "\n",
        "    def calculate_token_cost(self, model_name, prompt, response_text, usage_info=None):\n",
        "        \"\"\"Calculate the actual cost. For OpenRouter, relies on usage_info from API response.\"\"\"\n",
        "        total_cost = 0.0\n",
        "        error_message = None\n",
        "        if model_name in MODELS_CONFIG:\n",
        "            model_cfg = MODELS_CONFIG[model_name]\n",
        "            if usage_info:\n",
        "                input_tokens = usage_info.get(\"prompt_tokens\", 0)\n",
        "                output_tokens = usage_info.get(\"completion_tokens\", 0)\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "            else:\n",
        "                error_message = f\"Usage info not available for {model_name}. Cost is a rough estimate.\"\n",
        "                input_tokens = len(prompt) // 4\n",
        "                output_tokens = len(response_text) // 4 if response_text else 0\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "        else:\n",
        "            error_message = f\"Model {model_name} not found in config for cost calculation.\"\n",
        "        result = {\"total_cost\": total_cost}\n",
        "        if error_message:\n",
        "            result[\"error\"] = error_message\n",
        "            print(f\"Cost calculation warning for {model_name}: {error_message}\")\n",
        "        return result\n",
        "\n",
        "    def query_llm(self, model_name, prompt):\n",
        "        \"\"\"Query the specified LLM and return its response and cost.\"\"\"\n",
        "        answer_text, parsed_answer, cost_data = \"\", None, {}\n",
        "        try:\n",
        "            self.linucb_model.respect_rate_limit(model_name)\n",
        "            if model_name in MODELS_CONFIG:\n",
        "                api_response = openrouter_client.chat.completions.create(\n",
        "                    model=model_name,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                )\n",
        "                answer_text = api_response.choices[0].message.content.strip()\n",
        "                usage_info = api_response.usage.model_dump() if api_response.usage else None\n",
        "                cost_data = self.calculate_token_cost(model_name, prompt, answer_text, usage_info=usage_info)\n",
        "            else:\n",
        "                raise ValueError(f\"Model {model_name} is not configured in MODELS_CONFIG.\")\n",
        "            self.linucb_model.register_model_call(model_name)\n",
        "            parsed_answer = self.parse_llm_answer(answer_text)\n",
        "            return answer_text, parsed_answer, cost_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying LLM {model_name}: {e}\")\n",
        "            self.linucb_model.register_model_call(model_name)\n",
        "            cost_data = self.calculate_token_cost(model_name, prompt, \"\", usage_info=None)\n",
        "            return \"\", None, cost_data\n",
        "\n",
        "    def run_cascade_single_question(self, question):\n",
        "        \"\"\"Run the standard LinUCB cascade for a single question.\"\"\"\n",
        "        base_features = self.feature_extractor.extract_features(question)\n",
        "        failed_answers, failed_llm_ids, current_attempts_log = [], [], []\n",
        "        question_total_cost = 0.0\n",
        "        final_status = \"Failure\"\n",
        "\n",
        "        for i in range(1, self.cascade_length + 1):\n",
        "            print(f\"Step {i}\")\n",
        "            # 1. Construct feature vector with history\n",
        "            x_i = self.feature_extractor.construct_feature_vector(base_features, i, failed_answers, failed_llm_ids, self.linucb_model.model_name_to_index)\n",
        "            # 2. Select model based on UCB score\n",
        "            chosen_model, scores = self.linucb_model.select_model_ucb(x_i)\n",
        "            if not chosen_model:\n",
        "                print(\"Error: LinUCB failed to select a model.\")\n",
        "                break\n",
        "            print(f\"Selected: {chosen_model}, UCB: {scores[chosen_model]['ucb_score']:.4f}\")\n",
        "            # 3. Query the chosen model\n",
        "            prompt = self.format_prompt(question, failed_answers, failed_llm_ids)\n",
        "            raw_response, model_answer, cost_data = self.query_llm(chosen_model, prompt)\n",
        "            actual_cost = cost_data.get(\"total_cost\", 0.0)\n",
        "            question_total_cost += actual_cost\n",
        "            # 4. Grade the answer\n",
        "            is_correct = self.grade_with_gemma12b(model_answer, question['ground_truth_answer'])\n",
        "            reward = 1 if is_correct else 0\n",
        "            print(f\"Answer: {model_answer}, Correct: {is_correct}, Cost: ${actual_cost:.8f}\")\n",
        "            # 5. Update LinUCB model\n",
        "            self.linucb_model.update_reward_only(chosen_model, x_i, reward)\n",
        "            # 6. Log the attempt\n",
        "            current_attempts_log.append({\n",
        "                \"step\": i, \"chosen_model\": chosen_model, \"chosen_model_cost\": cost_data,\n",
        "                \"llm_answer\": model_answer, \"is_correct\": is_correct, \"reward_ri\": reward,\n",
        "                \"raw_response\": raw_response, \"scores_per_arm\": scores\n",
        "            })\n",
        "            # 7. Check for success\n",
        "            if is_correct:\n",
        "                final_status = \"Success\"\n",
        "                print(f\"Success in step {i}!\")\n",
        "                break\n",
        "            else:\n",
        "                failed_answers.append(model_answer if model_answer else \"ParsingFailed/NoAnswer\")\n",
        "                failed_llm_ids.append(chosen_model)\n",
        "\n",
        "        return {\n",
        "            \"question\": question[\"question\"], \"ground_truth_answer\": question[\"ground_truth_answer\"],\n",
        "            \"final_status\": final_status, \"total_cost\": question_total_cost,\n",
        "            \"steps_taken\": len(current_attempts_log), \"attempts\": current_attempts_log\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_math500_dataset(json_path):\n",
        "    \"\"\"Load and process the Math500 dataset\"\"\"\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            raw_data = json.load(f)\n",
        "\n",
        "        processed_dataset = []\n",
        "\n",
        "        # Assuming the problems are in a list under the \"test\" key\n",
        "        for item in raw_data[\"test\"]:\n",
        "            processed_dataset.append({\n",
        "                \"question\": item[\"problem\"],  # Map \"problem\" to \"question\"\n",
        "                \"options\": [],  # Math500 has no MCQs, so empty list\n",
        "                \"ground_truth_answer\": item[\"answer\"],  # Store the LaTeX answer\n",
        "                \"subject\": item.get(\"subject\", \"Unknown\"),  # Keep other useful metadata\n",
        "                \"level\": item.get(\"level\", \"Unknown\"),\n",
        "                \"unique_id\": item.get(\"unique_id\", \"Unknown\")\n",
        "            })\n",
        "\n",
        "        print(f\"Loaded {len(processed_dataset)} questions from {json_path}\")\n",
        "        return processed_dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Math500 dataset: {e}\")\n",
        "        return []\n",
        "\n",
        "def save_records_with_backup(records, json_path):\n",
        "    \"\"\"Save records to a JSON file with backup of previous file\"\"\"\n",
        "    # Create backup of existing file if it exists\n",
        "    if os.path.exists(json_path):\n",
        "        backup_path = json_path + BACKUP_SUFFIX\n",
        "        try:\n",
        "            os.replace(json_path, backup_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create backup: {e}\")\n",
        "\n",
        "    # Save new data\n",
        "    try:\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(records, f, indent=4)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving records: {e}\")\n",
        "\n",
        "        # Try to restore from backup if save failed\n",
        "        if os.path.exists(json_path + BACKUP_SUFFIX):\n",
        "            try:\n",
        "                os.replace(json_path + BACKUP_SUFFIX, json_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return False\n",
        "\n",
        "def initialize_json_files():\n",
        "    \"\"\"Initialize the JSON files for records with validation\"\"\"\n",
        "    if not os.path.exists(RECORDS_PATH):\n",
        "        with open(RECORDS_PATH, 'w') as f:\n",
        "            json.dump([], f)  # Empty array\n",
        "    else:\n",
        "        # Validate existing file\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            if not isinstance(data, list):\n",
        "                os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
        "                with open(RECORDS_PATH, 'w') as f:\n",
        "                    json.dump([], f)\n",
        "        except json.JSONDecodeError:\n",
        "            os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
        "            with open(RECORDS_PATH, 'w') as f:\n",
        "                json.dump([], f)\n",
        "\n",
        "def calculate_metrics(records):\n",
        "    \"\"\"Helper function to calculate metrics for a subset of records\"\"\"\n",
        "    if not records:\n",
        "        return {\n",
        "            \"total_questions\": 0,\n",
        "            \"successful_questions\": 0,\n",
        "            \"success_rate\": 0,\n",
        "            \"total_steps\": 0,\n",
        "            \"avg_steps\": 0,\n",
        "            \"total_cost\": 0,\n",
        "            \"avg_cost\": 0,\n",
        "            \"avg_cost_success\": 0,\n",
        "            \"successes_by_position\": [0] * CASCADE_LENGTH,\n",
        "            \"per_position_success\": [0] * CASCADE_LENGTH,\n",
        "            \"model_metrics\": {}\n",
        "        }\n",
        "\n",
        "    total_questions = len(records)\n",
        "    successful_questions = sum(1 for r in records if r[\"final_status\"] == \"Success\")\n",
        "    success_rate = successful_questions / total_questions if total_questions > 0 else 0\n",
        "\n",
        "    # Calculate success by position\n",
        "    successes_by_position = [0] * CASCADE_LENGTH\n",
        "    for record in records:\n",
        "        if record[\"final_status\"] == \"Success\":\n",
        "            position = len(record[\"attempts\"]) - 1  # 0-indexed\n",
        "            if position < CASCADE_LENGTH:\n",
        "                successes_by_position[position] += 1\n",
        "\n",
        "    per_position_success = [count/total_questions for count in successes_by_position]\n",
        "\n",
        "    # Calculate average steps and costs\n",
        "    total_steps = sum(r[\"steps_taken\"] for r in records)\n",
        "    total_cost = sum(r[\"total_cost\"] for r in records)\n",
        "\n",
        "    avg_steps = total_steps / total_questions if total_questions > 0 else 0\n",
        "    avg_cost = total_cost / total_questions if total_questions > 0 else 0\n",
        "\n",
        "    # Calculate average cost for successful questions only\n",
        "    if successful_questions > 0:\n",
        "        success_cost = sum(r[\"total_cost\"] for r in records if r[\"final_status\"] == \"Success\")\n",
        "        avg_cost_success = success_cost / successful_questions\n",
        "    else:\n",
        "        avg_cost_success = 0\n",
        "\n",
        "    # Analyze model performance\n",
        "    model_metrics = {model: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0, \"by_position\": defaultdict(lambda: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0})}\n",
        "                    for model in AVAILABLE_LLMS}\n",
        "\n",
        "    for record in records:\n",
        "        for attempt in record[\"attempts\"]:\n",
        "            position = attempt[\"step\"]\n",
        "            model = attempt[\"chosen_model\"]\n",
        "            is_correct = attempt[\"is_correct\"]\n",
        "            cost = attempt[\"chosen_model_cost\"][\"total_cost\"]\n",
        "\n",
        "            # Update overall stats\n",
        "            model_metrics[model][\"calls\"] += 1\n",
        "            model_metrics[model][\"total_cost\"] += cost\n",
        "            if is_correct:\n",
        "                model_metrics[model][\"successes\"] += 1\n",
        "\n",
        "            # Update position-specific stats\n",
        "            model_metrics[model][\"by_position\"][position][\"calls\"] += 1\n",
        "            model_metrics[model][\"by_position\"][position][\"total_cost\"] += cost\n",
        "            if is_correct:\n",
        "                model_metrics[model][\"by_position\"][position][\"successes\"] += 1\n",
        "\n",
        "    # Calculate success rates and average costs for models\n",
        "    for model, data in model_metrics.items():\n",
        "        if data[\"calls\"] > 0:\n",
        "            data[\"success_rate\"] = data[\"successes\"] / data[\"calls\"]\n",
        "            data[\"avg_cost\"] = data[\"total_cost\"] / data[\"calls\"]\n",
        "        else:\n",
        "            data[\"success_rate\"] = 0\n",
        "            data[\"avg_cost\"] = 0\n",
        "\n",
        "        for position, pos_data in data[\"by_position\"].items():\n",
        "            if pos_data[\"calls\"] > 0:\n",
        "                pos_data[\"success_rate\"] = pos_data[\"successes\"] / pos_data[\"calls\"]\n",
        "                pos_data[\"avg_cost\"] = pos_data[\"total_cost\"] / pos_data[\"calls\"]\n",
        "            else:\n",
        "                pos_data[\"success_rate\"] = 0\n",
        "                pos_data[\"avg_cost\"] = 0\n",
        "\n",
        "    return {\n",
        "        \"total_questions\": total_questions,\n",
        "        \"successful_questions\": successful_questions,\n",
        "        \"success_rate\": success_rate,\n",
        "        \"total_steps\": total_steps,\n",
        "        \"avg_steps\": avg_steps,\n",
        "        \"total_cost\": total_cost,\n",
        "        \"avg_cost\": avg_cost,\n",
        "        \"avg_cost_success\": avg_cost_success,\n",
        "        \"successes_by_position\": successes_by_position,\n",
        "        \"per_position_success\": per_position_success,\n",
        "        \"model_metrics\": model_metrics\n",
        "    }\n",
        "\n",
        "def analyze_results(records):\n",
        "    \"\"\"Analyze and print results from the records, separating train and test sets\"\"\"\n",
        "    if not records:\n",
        "        print(\"No records to analyze\")\n",
        "        return\n",
        "\n",
        "    # Separate records into train and test sets\n",
        "    train_records = [r for r in records if r.get(\"phase\") == \"train\"]\n",
        "    test_records = [r for r in records if r.get(\"phase\") == \"test\"]\n",
        "    overall_records = records\n",
        "\n",
        "    print(f\"Train Records: {len(train_records)}\")\n",
        "    print(f\"Test Records: {len(test_records)}\")\n",
        "    print(f\"Total Records: {len(overall_records)}\")\n",
        "\n",
        "    # Calculate metrics for each set\n",
        "    train_metrics = calculate_metrics(train_records)\n",
        "    test_metrics = calculate_metrics(test_records)\n",
        "    overall_metrics = calculate_metrics(overall_records)\n",
        "\n",
        "    # Analyze batch performance\n",
        "    batch_metrics = []\n",
        "    for i in range(0, len(records), BATCH_SIZE):\n",
        "        batch = records[i:i+BATCH_SIZE]\n",
        "        batch_success = sum(1 for r in batch if r[\"final_status\"] == \"Success\")\n",
        "        batch_success_rate = batch_success / len(batch) if batch else 0\n",
        "        batch_cost = sum(r[\"total_cost\"] for r in batch)\n",
        "        batch_avg_cost = batch_cost / len(batch) if batch else 0\n",
        "        batch_train = sum(1 for r in batch if r.get(\"phase\") == \"train\")\n",
        "        batch_test = sum(1 for r in batch if r.get(\"phase\") == \"test\")\n",
        "\n",
        "        batch_metrics.append({\n",
        "            \"batch_idx\": i // BATCH_SIZE,\n",
        "            \"batch_size\": len(batch),\n",
        "            \"train_count\": batch_train,\n",
        "            \"test_count\": batch_test,\n",
        "            \"success_count\": batch_success,\n",
        "            \"success_rate\": batch_success_rate,\n",
        "            \"total_cost\": batch_cost,\n",
        "            \"avg_cost\": batch_avg_cost\n",
        "        })\n",
        "\n",
        "    # Generate summary text\n",
        "    summary = \"=== LinUCB CASCADE WITH MATH500 DATASET ===\\n\\n\"\n",
        "    summary += f\"Train Ratio: {TRAIN_RATIO*100}%\\n\"\n",
        "    summary += f\"LinUCB updated on BOTH Train and Test sets.\\n\"\n",
        "    summary += f\"Batch Size: {BATCH_SIZE}\\n\\n\"\n",
        "\n",
        "    # TRAIN SET RESULTS\n",
        "    summary += \"=== TRAIN SET RESULTS ===\\n\"\n",
        "    summary += f\"Total Train Questions: {train_metrics['total_questions']}\\n\"\n",
        "    summary += f\"Success Rate: {train_metrics['success_rate']:.4f}\\n\"\n",
        "    summary += f\"Average Steps: {train_metrics['avg_steps']:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${train_metrics['avg_cost']:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${train_metrics['avg_cost_success']:.8f}\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(train_metrics['per_position_success']):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\nTrain Set Model Performance:\\n\"\n",
        "    for model, metrics in train_metrics['model_metrics'].items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"\\n{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "            summary += \"  By Position:\\n\"\n",
        "            for position, pos_data in sorted(metrics[\"by_position\"].items()):\n",
        "                if pos_data[\"calls\"] > 0:\n",
        "                    summary += f\"    Pos {position}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    # TEST SET RESULTS\n",
        "    summary += \"\\n\\n=== TEST SET RESULTS ===\\n\"\n",
        "    summary += f\"Total Test Questions: {test_metrics['total_questions']}\\n\"\n",
        "    summary += f\"Success Rate: {test_metrics['success_rate']:.4f}\\n\"\n",
        "    summary += f\"Average Steps: {test_metrics['avg_steps']:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${test_metrics['avg_cost']:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${test_metrics['avg_cost_success']:.8f}\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(test_metrics['per_position_success']):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\nTest Set Model Performance:\\n\"\n",
        "    for model, metrics in test_metrics['model_metrics'].items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"\\n{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "            summary += \"  By Position:\\n\"\n",
        "            for position, pos_data in sorted(metrics[\"by_position\"].items()):\n",
        "                if pos_data[\"calls\"] > 0:\n",
        "                    summary += f\"    Pos {position}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    # OVERALL RESULTS\n",
        "    summary += \"\\n\\n=== OVERALL RESULTS (TRAIN + TEST) ===\\n\"\n",
        "    summary += f\"Total Overall Questions: {overall_metrics['total_questions']}\\n\"\n",
        "    summary += f\"Success Rate: {overall_metrics['success_rate']:.4f}\\n\"\n",
        "    summary += f\"Average Steps: {overall_metrics['avg_steps']:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${overall_metrics['avg_cost']:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${overall_metrics['avg_cost_success']:.8f}\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(overall_metrics['per_position_success']):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\nOverall Model Performance:\\n\"\n",
        "    for model, metrics in overall_metrics['model_metrics'].items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"\\n{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "            summary += \"  By Position:\\n\"\n",
        "            for position, pos_data in sorted(metrics[\"by_position\"].items()):\n",
        "                if pos_data[\"calls\"] > 0:\n",
        "                    summary += f\"    Pos {position}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== BATCH PERFORMANCE ===\\n\"\n",
        "    for batch in batch_metrics:\n",
        "        summary += f\"Batch {batch['batch_idx']}: {batch['success_count']}/{batch['batch_size']} = {batch['success_rate']:.4f}, \" \\\n",
        "                  f\"Train/Test: {batch['train_count']}/{batch['test_count']}, \" \\\n",
        "                  f\"Total Cost: ${batch['total_cost']:.8f}, Avg Cost: ${batch['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += f\"\\nTotal Overall Cost (All Questions): ${overall_metrics['total_cost']:.8f}\\n\"\n",
        "    summary += f\"Total Train Cost: ${train_metrics['total_cost']:.8f}\\n\"\n",
        "    summary += f\"Total Test Cost: ${test_metrics['total_cost']:.8f}\\n\"\n",
        "\n",
        "    # Add interpretation note about test set updates\n",
        "    summary += \"\\n=== IMPORTANT NOTES ON INTERPRETATION ===\\n\"\n",
        "    summary += \"Since the LinUCB model is updated using data from both the train and test sets,\\n\"\n",
        "    summary += \"the test set performance metrics do not represent the performance of a fixed,\\n\"\n",
        "    summary += \"pre-trained model on unseen data. Instead, they reflect the model's performance\\n\"\n",
        "    summary += \"while it is still learning and adapting to the test data (online evaluation).\\n\"\n",
        "    summary += \"This setup allows us to observe how the bandit algorithm performs and adapts\\n\"\n",
        "    summary += \"over time across the entire dataset, comparing performance between an initial\\n\"\n",
        "    summary += \"phase (train) and a later phase (test).\\n\"\n",
        "\n",
        "    # Print and save summary\n",
        "    print(summary)\n",
        "\n",
        "    with open(SUMMARY_STATS_PATH, 'w') as f:\n",
        "        f.write(summary)\n"
      ],
      "metadata": {
        "id": "llh_P_4Yh7kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    print(\"Starting LinUCB Cascade with Math500 Dataset\")\n",
        "    initialize_json_files()\n",
        "    dataset_full = load_math500_dataset(INPUT_JSON)\n",
        "    if not dataset_full:\n",
        "        print(\"Dataset is empty or could not be loaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        num_to_process = int(input(f\"Enter the number of questions to process (max {len(dataset_full)}): \"))\n",
        "        num_to_process = min(num_to_process, len(dataset_full))\n",
        "    except ValueError:\n",
        "        num_to_process = len(dataset_full)\n",
        "        print(f\"Invalid input. Using all {num_to_process} questions.\")\n",
        "\n",
        "    random.seed(42) # for reproducible shuffle\n",
        "    random.shuffle(dataset_full)\n",
        "    dataset = dataset_full[:num_to_process]\n",
        "    train_size = int(TRAIN_RATIO * len(dataset))\n",
        "    print(f\"Using {len(dataset)} questions: {train_size} train, {len(dataset) - train_size} test.\")\n",
        "\n",
        "    feature_extractor = FeatureExtractor()\n",
        "    feature_extractor.initialize(dataset)\n",
        "    linucb_model = LinUCBModel(model_names=AVAILABLE_LLMS)\n",
        "    cascade = BatchBudgetCascade(feature_extractor, linucb_model)\n",
        "\n",
        "    all_records = []\n",
        "    start_idx = 0\n",
        "    if os.path.exists(RECORDS_PATH):\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f: all_records = json.load(f)\n",
        "            processed_questions = {r[\"question\"] for r in all_records}\n",
        "            dataset = [q for q in dataset if q[\"question\"] not in processed_questions]\n",
        "            print(f\"Loaded {len(all_records)} existing records. {len(dataset)} new questions to process.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load existing records: {e}\")\n",
        "            all_records = []\n",
        "\n",
        "    if os.path.exists(LINUCB_MODEL_PATH):\n",
        "        linucb_model.load_model_state(LINUCB_MODEL_PATH)\n",
        "\n",
        "    try:\n",
        "        for i, question in enumerate(dataset):\n",
        "            phase = \"test\" if i >= train_size else \"train\"\n",
        "            print(f\"\\n--- Q{i+1}/{len(dataset)} ({phase}) ---\")\n",
        "            print(f\"Question: {question['question'][:100]}...\")\n",
        "            question_record = cascade.run_cascade_single_question(question)\n",
        "            question_record[\"phase\"] = phase\n",
        "            all_records.append(question_record)\n",
        "\n",
        "            if (i + 1) % UPDATE_FREQUENCY == 0:\n",
        "                save_records_with_backup(all_records, RECORDS_PATH)\n",
        "                linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProcessing interrupted. Saving progress...\")\n",
        "    finally:\n",
        "        save_records_with_backup(all_records, RECORDS_PATH)\n",
        "        linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "        analyze_results(all_records)\n",
        "        print(\"\\nLinUCB Cascade with Math500 Dataset completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "SN8rbZZ0hPvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}